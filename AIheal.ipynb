{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXMa3ikL+gfJbp2Dmy3a3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NXdevansh/healX-/blob/main/AIheal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCBuAGlQ_uaD",
        "outputId": "3ecd7bb9-9cbb-466a-a87d-0918d51030ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: False\n",
            "Random seed set to 42\n"
          ]
        }
      ],
      "source": [
        "# Install necessary Python libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch_geometric\n",
        "!pip install transformers\n",
        "!pip install networkx\n",
        "!pip install numpy pandas scikit-learn\n",
        "!pip install matplotlib seaborn plotly\n",
        "!pip install tqdm\n",
        "\n",
        "# Check if GPU is available\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    print(f\"Random seed set to {seed}\")\n",
        "\n",
        "set_seed()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class SmartWatchDataGenerator:\n",
        "    def __init__(self, n_days=7, sample_rate_minutes=1):\n",
        "        \"\"\"\n",
        "        Generate synthetic smartwatch data streams\n",
        "\n",
        "        Args:\n",
        "            n_days: Number of days of data to simulate\n",
        "            sample_rate_minutes: Data frequency in minutes\n",
        "        \"\"\"\n",
        "        self.n_days = n_days\n",
        "        self.sample_rate_minutes = sample_rate_minutes\n",
        "        self.n_samples = int((n_days * 24 * 60) / sample_rate_minutes)\n",
        "        self.start_time = datetime.now() - timedelta(days=n_days)\n",
        "\n",
        "    def generate_timestamps(self):\n",
        "        \"\"\"Generate evenly spaced timestamps\"\"\"\n",
        "        timestamps = [\n",
        "            self.start_time + timedelta(minutes=i * self.sample_rate_minutes)\n",
        "            for i in range(self.n_samples)\n",
        "        ]\n",
        "        return timestamps\n",
        "\n",
        "    def _generate_heart_data(self, activity_levels):\n",
        "        \"\"\"Generate PPG/ECG/HR data\"\"\"\n",
        "        # Base heart rate varies with activity\n",
        "        base_hr = 60 + 40 * activity_levels\n",
        "\n",
        "        # Add circadian rhythm (lower at night)\n",
        "        timestamps = self.generate_timestamps()\n",
        "        hours = np.array([t.hour for t in timestamps])\n",
        "        circadian = -10 * np.cos((hours - 14) * 2 * np.pi / 24)\n",
        "\n",
        "        # Add noise and occasional irregularities\n",
        "        hr = base_hr + circadian + np.random.normal(0, 5, self.n_samples)\n",
        "\n",
        "        # Create occasional arrhythmia\n",
        "        arrhythmia_mask = np.random.random(self.n_samples) < 0.005\n",
        "        hr[arrhythmia_mask] += np.random.choice([-20, 20], size=sum(arrhythmia_mask))\n",
        "\n",
        "        # Generate synthetic PPG wave\n",
        "        ppg = []\n",
        "        ecg = []\n",
        "\n",
        "        for h in hr:\n",
        "            # Simplistic PPG wave (just for demonstration)\n",
        "            cycle_length = 60 / h  # seconds per beat\n",
        "            points_per_cycle = int(60 / (h * self.sample_rate_minutes)) + 1\n",
        "            t = np.linspace(0, 2*np.pi, points_per_cycle)\n",
        "            ppg_cycle = np.sin(t) * 0.4 + 0.6 + np.random.normal(0, 0.05, len(t))\n",
        "            ppg.append(ppg_cycle[0])\n",
        "\n",
        "            # Simplistic ECG wave (just for demonstration)\n",
        "            ecg_cycle = np.zeros_like(t)\n",
        "            p_wave = 0.2 * np.exp(-(t-0.5)**2/0.1)\n",
        "            qrs = 0.8 * np.exp(-(t-np.pi)**2/0.02) - 0.3 * np.exp(-(t-(np.pi+0.2))**2/0.02)\n",
        "            t_wave = 0.3 * np.exp(-(t-(np.pi+1.0))**2/0.1)\n",
        "            ecg_cycle = p_wave + qrs + t_wave + np.random.normal(0, 0.05, len(t))\n",
        "            ecg.append(ecg_cycle[0])\n",
        "\n",
        "        return hr, np.array(ppg), np.array(ecg)\n",
        "\n",
        "    def _generate_spo2_data(self, activity_levels, hr):\n",
        "        \"\"\"Generate SpO2 data\"\"\"\n",
        "        # Baseline SpO2 is typically 95-99%\n",
        "        baseline = 97 + np.random.normal(0, 1, self.n_samples)\n",
        "\n",
        "        # SpO2 drops slightly with intense activity\n",
        "        activity_effect = -1 * (activity_levels > 0.7) * activity_levels\n",
        "\n",
        "        # Create occasional hypoxic events (more likely during sleep)\n",
        "        timestamps = self.generate_timestamps()\n",
        "        hours = np.array([t.hour for t in timestamps])\n",
        "        is_night = (hours >= 22) | (hours <= 6)\n",
        "\n",
        "        hypoxic_mask = (np.random.random(self.n_samples) < 0.01) & is_night\n",
        "        hypoxic_events = np.zeros(self.n_samples)\n",
        "        hypoxic_events[hypoxic_mask] = -np.random.randint(5, 15, size=sum(hypoxic_mask))\n",
        "\n",
        "        spo2 = baseline + activity_effect + hypoxic_events\n",
        "\n",
        "        # Cap values to realistic range\n",
        "        spo2 = np.clip(spo2, 70, 100)\n",
        "\n",
        "        return spo2\n",
        "\n",
        "    def _generate_activity_data(self):\n",
        "        \"\"\"Generate activity levels and accelerometer data\"\"\"\n",
        "        # Create activity pattern with daily routine\n",
        "        timestamps = self.generate_timestamps()\n",
        "        hours = np.array([t.hour for t in timestamps])\n",
        "\n",
        "        # Base activity level follows a pattern\n",
        "        # Higher during daytime, peaks in morning and evening (exercise)\n",
        "        activity_base = np.zeros(self.n_samples)\n",
        "\n",
        "        # Morning activity (6-8 AM)\n",
        "        morning_mask = (hours >= 6) & (hours < 8)\n",
        "        activity_base[morning_mask] = 0.7\n",
        "\n",
        "        # Daytime activity (8 AM - 6 PM)\n",
        "        day_mask = (hours >= 8) & (hours < 18)\n",
        "        activity_base[day_mask] = 0.4\n",
        "\n",
        "        # Evening exercise (6-8 PM)\n",
        "        evening_mask = (hours >= 18) & (hours < 20)\n",
        "        activity_base[evening_mask] = 0.8\n",
        "\n",
        "        # Evening wind down (8-11 PM)\n",
        "        wind_down_mask = (hours >= 20) & (hours < 23)\n",
        "        activity_base[wind_down_mask] = 0.3\n",
        "\n",
        "        # Night sleep (11 PM - 6 AM)\n",
        "        night_mask = (hours >= 23) | (hours < 6)\n",
        "        activity_base[night_mask] = 0.05\n",
        "\n",
        "        # Add randomness\n",
        "        activity_levels = activity_base + np.random.normal(0, 0.1, self.n_samples)\n",
        "        activity_levels = np.clip(activity_levels, 0, 1)\n",
        "\n",
        "        # Generate steps based on activity\n",
        "        steps_per_minute = activity_levels * 120  # Max ~120 steps per minute\n",
        "        steps = np.random.poisson(steps_per_minute)\n",
        "\n",
        "        # Generate accelerometer data (3-axis)\n",
        "        accel_x = activity_levels * np.random.normal(0, 1, self.n_samples)\n",
        "        accel_y = activity_levels * np.random.normal(0, 1, self.n_samples)\n",
        "        accel_z = activity_levels * np.random.normal(-1, 1, self.n_samples) - 9.8  # gravity\n",
        "\n",
        "        return activity_levels, steps, np.column_stack((accel_x, accel_y, accel_z))\n",
        "\n",
        "    def _generate_sleep_data(self, activity_levels):\n",
        "        \"\"\"Generate sleep staging data\"\"\"\n",
        "        # Low activity typically means sleep\n",
        "        is_likely_sleeping = activity_levels < 0.1\n",
        "\n",
        "        # Sleep staging (0=wake, 1=light, 2=deep, 3=REM)\n",
        "        sleep_stage = np.zeros(self.n_samples)\n",
        "\n",
        "        # Set sleep stages for likely sleep periods\n",
        "        sleep_stage[is_likely_sleeping] = np.random.choice(\n",
        "            [1, 2, 3],\n",
        "            size=sum(is_likely_sleeping),\n",
        "            p=[0.5, 0.3, 0.2]\n",
        "        )\n",
        "\n",
        "        return sleep_stage\n",
        "\n",
        "    def _generate_stress_hrv(self, activity_levels, hr):\n",
        "        \"\"\"Generate stress scores and HRV data\"\"\"\n",
        "        # HRV is typically higher during rest and lower during stress\n",
        "        base_hrv = 50 - 30 * activity_levels\n",
        "\n",
        "        # Add circadian component and noise\n",
        "        timestamps = self.generate_timestamps()\n",
        "        hours = np.array([t.hour for t in timestamps])\n",
        "        circadian = 10 * np.cos((hours - 2) * 2 * np.pi / 24)\n",
        "\n",
        "        hrv = base_hrv + circadian + np.random.normal(0, 5, self.n_samples)\n",
        "        hrv = np.clip(hrv, 10, 100)\n",
        "\n",
        "        # Stress score is inverse of normalized HRV\n",
        "        stress = 100 - (hrv - 10) / 0.9\n",
        "        stress = np.clip(stress, 0, 100)\n",
        "\n",
        "        return stress, hrv\n",
        "\n",
        "    def _generate_user_meta(self):\n",
        "        \"\"\"Generate mock user metadata\"\"\"\n",
        "        age = np.random.randint(25, 65)\n",
        "        gender = np.random.choice(['M', 'F'])\n",
        "        height = np.random.normal(170, 10) if gender == 'M' else np.random.normal(165, 8)\n",
        "        weight = np.random.normal(75, 10) if gender == 'M' else np.random.normal(65, 8)\n",
        "        bmi = weight / ((height/100) ** 2)\n",
        "\n",
        "        region_choices = ['US-East', 'US-West', 'Europe', 'Asia']\n",
        "        region = np.random.choice(region_choices)\n",
        "\n",
        "        # Generate pre-existing conditions\n",
        "        conditions = []\n",
        "        if np.random.random() < 0.1:\n",
        "            conditions.append('hypertension')\n",
        "        if np.random.random() < 0.08:\n",
        "            conditions.append('diabetes')\n",
        "        if np.random.random() < 0.05:\n",
        "            conditions.append('asthma')\n",
        "\n",
        "        return {\n",
        "            'age': age,\n",
        "            'gender': gender,\n",
        "            'height': height,\n",
        "            'weight': weight,\n",
        "            'bmi': bmi,\n",
        "            'region': region,\n",
        "            'conditions': conditions\n",
        "        }\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\"Generate complete dataset\"\"\"\n",
        "        # First generate activity since other metrics depend on it\n",
        "        activity_levels, steps, accel = self._generate_activity_data()\n",
        "\n",
        "        # Generate physiological measures\n",
        "        hr, ppg, ecg = self._generate_heart_data(activity_levels)\n",
        "        spo2 = self._generate_spo2_data(activity_levels, hr)\n",
        "        sleep_stage = self._generate_sleep_data(activity_levels)\n",
        "        stress, hrv = self._generate_stress_hrv(activity_levels, hr)\n",
        "\n",
        "        # Create DataFrame\n",
        "        timestamps = self.generate_timestamps()\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'timestamp': timestamps,\n",
        "            'hr': hr,\n",
        "            'ppg': list(ppg),  # Store as list since each might be different length\n",
        "            'ecg': list(ecg),  # Store as list since each might be different length\n",
        "            'spo2': spo2,\n",
        "            'activity_level': activity_levels,\n",
        "            'steps': steps,\n",
        "            'accel_x': accel[:, 0],\n",
        "            'accel_y': accel[:, 1],\n",
        "            'accel_z': accel[:, 2],\n",
        "            'sleep_stage': sleep_stage,\n",
        "            'stress': stress,\n",
        "            'hrv': hrv\n",
        "        })\n",
        "\n",
        "        # Generate user metadata\n",
        "        user_meta = self._generate_user_meta()\n",
        "\n",
        "        return df, user_meta\n",
        "\n",
        "    def plot_sample_data(self, df, figsize=(15, 20)):\n",
        "        \"\"\"Plot sample of the generated data\"\"\"\n",
        "        fig, axes = plt.subplots(5, 1, figsize=figsize)\n",
        "\n",
        "        # Sample 1440 points (1 day)\n",
        "        sample = df.iloc[:1440]\n",
        "\n",
        "        # HR and SpO2\n",
        "        ax1 = axes[0]\n",
        "        ax1.plot(sample['timestamp'], sample['hr'], 'r-', label='Heart Rate')\n",
        "        ax1.set_ylabel('Heart Rate (bpm)')\n",
        "        ax1.set_title('Heart Rate over Time')\n",
        "        ax1.legend(loc='upper left')\n",
        "\n",
        "        ax1b = ax1.twinx()\n",
        "        ax1b.plot(sample['timestamp'], sample['spo2'], 'b-', alpha=0.7, label='SpO2')\n",
        "        ax1b.set_ylabel('SpO2 (%)')\n",
        "        ax1b.legend(loc='upper right')\n",
        "\n",
        "        # Activity and Steps\n",
        "        ax2 = axes[1]\n",
        "        ax2.plot(sample['timestamp'], sample['activity_level'], 'g-', label='Activity Level')\n",
        "        ax2.set_ylabel('Activity Level (0-1)')\n",
        "        ax2.set_title('Activity Level and Steps')\n",
        "        ax2.legend(loc='upper left')\n",
        "\n",
        "        ax2b = ax2.twinx()\n",
        "        ax2b.plot(sample['timestamp'], sample['steps'], 'k-', alpha=0.3, label='Steps')\n",
        "        ax2b.set_ylabel('Steps per minute')\n",
        "        ax2b.legend(loc='upper right')\n",
        "\n",
        "        # Sleep Stage\n",
        "        ax3 = axes[2]\n",
        "        ax3.plot(sample['timestamp'], sample['sleep_stage'], 'b-', drawstyle='steps-post')\n",
        "        ax3.set_ylabel('Sleep Stage')\n",
        "        ax3.set_yticks([0, 1, 2, 3])\n",
        "        ax3.set_yticklabels(['Wake', 'Light', 'Deep', 'REM'])\n",
        "        ax3.set_title('Sleep Stages')\n",
        "\n",
        "        # Stress and HRV\n",
        "        ax4 = axes[3]\n",
        "        ax4.plot(sample['timestamp'], sample['stress'], 'r-', label='Stress')\n",
        "        ax4.set_ylabel('Stress (0-100)')\n",
        "        ax4.set_title('Stress and HRV')\n",
        "        ax4.legend(loc='upper left')\n",
        "\n",
        "        ax4b = ax4.twinx()\n",
        "        ax4b.plot(sample['timestamp'], sample['hrv'], 'g-', alpha=0.7, label='HRV')\n",
        "        ax4b.set_ylabel('HRV (ms)')\n",
        "        ax4b.legend(loc='upper right')\n",
        "\n",
        "        # Accelerometer\n",
        "        ax5 = axes[4]\n",
        "        ax5.plot(sample['timestamp'], sample['accel_x'], 'r-', alpha=0.5, label='X')\n",
        "        ax5.plot(sample['timestamp'], sample['accel_y'], 'g-', alpha=0.5, label='Y')\n",
        "        ax5.plot(sample['timestamp'], sample['accel_z'], 'b-', alpha=0.5, label='Z')\n",
        "        ax5.set_ylabel('Acceleration (m/s²)')\n",
        "        ax5.set_title('Accelerometer Data')\n",
        "        ax5.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "# Generate and save data\n",
        "generator = SmartWatchDataGenerator(n_days=14)  # 2 weeks of data\n",
        "data, user_meta = generator.generate_data()\n",
        "\n",
        "print(f\"Generated {len(data)} data points\")\n",
        "print(f\"User metadata: {user_meta}\")\n",
        "\n",
        "# Plot sample data\n",
        "fig = generator.plot_sample_data(data)\n",
        "plt.show()\n",
        "\n",
        "# Save the data for later use\n",
        "data.to_pickle('smartwatch_data.pkl')\n",
        "import json\n",
        "with open('user_meta.json', 'w') as f:\n",
        "    json.dump(user_meta, f)\n",
        "\n",
        "print(\"Data saved to 'smartwatch_data.pkl' and 'user_meta.json'\")\n",
        "\n",
        "# Display first few rows\n",
        "data.head()"
      ],
      "metadata": {
        "id": "eTVilBVN_1nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Load data\n",
        "data = pd.read_pickle('smartwatch_data.pkl')\n",
        "print(f\"Loaded {len(data)} data points\")\n",
        "\n",
        "class SmartWatchDataset(Dataset):\n",
        "    def __init__(self, df, window_size=60, stride=10):\n",
        "        \"\"\"\n",
        "        Create windowed smartwatch dataset for self-supervised learning\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with smartwatch data\n",
        "            window_size: Window size in minutes\n",
        "            stride: Stride for window creation in minutes\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # Create windows\n",
        "        self.window_indices = []\n",
        "        for i in range(0, len(df) - window_size, stride):\n",
        "            self.window_indices.append((i, i + window_size))\n",
        "\n",
        "        print(f\"Created {len(self.window_indices)} windows from {len(df)} data points\")\n",
        "\n",
        "        # Features to use (excluding 'ppg' and 'ecg' which need special handling)\n",
        "        self.features = ['hr', 'spo2', 'activity_level', 'steps',\n",
        "                         'accel_x', 'accel_y', 'accel_z', 'sleep_stage',\n",
        "                         'stress', 'hrv']\n",
        "\n",
        "        # Standardize numerical features\n",
        "        self.scalers = {}\n",
        "        for feature in self.features:\n",
        "            self.scalers[feature] = StandardScaler()\n",
        "            self.df[feature] = self.scalers[feature].fit_transform(self.df[[feature]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.window_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx, end_idx = self.window_indices[idx]\n",
        "        window_data = self.df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Extract features as tensors\n",
        "        feature_data = torch.tensor(\n",
        "            window_data[self.features].values,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Create a mask for self-supervised learning\n",
        "        # Randomly mask between 10-20% of all values\n",
        "        mask_rate = np.random.uniform(0.1, 0.2)\n",
        "        mask = torch.rand_like(feature_data) < mask_rate\n",
        "\n",
        "        # Create masked data (replace masked values with zeros)\n",
        "        masked_data = feature_data.clone()\n",
        "        masked_data[mask] = 0.0\n",
        "\n",
        "        # Calculate timestamp differences for temporal information\n",
        "        timestamps = window_data['timestamp'].values\n",
        "        time_diffs = np.array([(t - timestamps[0]).total_seconds() / 3600.0  # Hours\n",
        "                               for t in timestamps])\n",
        "        time_tensor = torch.tensor(time_diffs, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        # Add time information to features\n",
        "        features_with_time = torch.cat([masked_data, time_tensor], dim=1)\n",
        "\n",
        "        return {\n",
        "            'masked_data': features_with_time,  # Input: masked data with time\n",
        "            'original_data': feature_data,  # Target: original data to reconstruct\n",
        "            'mask': mask,  # The mask that was applied\n",
        "            'start_idx': start_idx,  # Starting index in original DataFrame\n",
        "            'end_idx': end_idx  # Ending index in original DataFrame\n",
        "        }\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return x\n",
        "\n",
        "class SmartWatchTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, nhead=8,\n",
        "                 num_layers=4, dropout=0.1, output_dim=200):\n",
        "        super(SmartWatchTransformer, self).__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
        "\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=hidden_dim*4,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "\n",
        "        # Embedding projection for getting the 200-dim latent representation\n",
        "        self.embedding_projection = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        # Reconstruction head for predicting original values\n",
        "        self.reconstruction_head = nn.Linear(output_dim, input_dim - 1)  # -1 because we added time\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [seq_len, batch_size, feature_dim]\n",
        "        src = self.input_projection(src)\n",
        "        src = self.pos_encoder(src)\n",
        "        memory = self.transformer_encoder(src)\n",
        "\n",
        "        # Get embeddings\n",
        "        embeddings = self.embedding_projection(memory)\n",
        "\n",
        "        # Reconstruct the original data (without time feature)\n",
        "        reconstructed = self.reconstruction_head(embeddings)\n",
        "\n",
        "        return embeddings, reconstructed\n",
        "\n",
        "class TemporalContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.similarity = nn.CosineSimilarity(dim=-1)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        # embeddings shape: [seq_len, batch_size, embed_dim]\n",
        "        seq_len, batch_size, _ = embeddings.shape\n",
        "\n",
        "        # Calculate similarity between each pair of positions\n",
        "        # Reshape embeddings to [seq_len*batch_size, embed_dim]\n",
        "        flat_embeddings = embeddings.reshape(-1, embeddings.size(2))\n",
        "\n",
        "        # Create positive pairs (adjacent timestamps)\n",
        "        pos_indices = []\n",
        "        for i in range(seq_len-1):\n",
        "            for b in range(batch_size):\n",
        "                pos_indices.append((i*batch_size + b, (i+1)*batch_size + b))\n",
        "\n",
        "        # Sample negative pairs (timestamps far apart)\n",
        "        neg_indices = []\n",
        "        for i in range(seq_len):\n",
        "            for b in range(batch_size):\n",
        "                current_idx = i*batch_size + b\n",
        "\n",
        "                # Consider any position at least 10 steps away as negative\n",
        "                neg_candidates = []\n",
        "                for j in range(seq_len):\n",
        "                    if abs(j - i) > 10:  # Far enough to be negative\n",
        "                        for c in range(batch_size):\n",
        "                            neg_candidates.append(j*batch_size + c)\n",
        "\n",
        "                # Sample 10 negative pairs per positive\n",
        "                if neg_candidates:\n",
        "                    sampled_negs = np.random.choice(neg_candidates,\n",
        "                                                   size=min(10, len(neg_candidates)),\n",
        "                                                   replace=False)\n",
        "                    for neg_idx in sampled_negs:\n",
        "                        neg_indices.append((current_idx, neg_idx))\n",
        "\n",
        "        # Calculate similarity for positive pairs\n",
        "        pos_sim = torch.stack([self.similarity(flat_embeddings[i], flat_embeddings[j])\n",
        "                              for i, j in pos_indices])\n",
        "        pos_sim = pos_sim / self.temperature\n",
        "\n",
        "        # Calculate similarity for negative pairs\n",
        "        neg_sim = torch.stack([self.similarity(flat_embeddings[i], flat_embeddings[j])\n",
        "                              for i, j in neg_indices])\n",
        "        neg_sim = neg_sim / self.temperature\n",
        "\n",
        "        # InfoNCE loss\n",
        "        pos_loss = -pos_sim.mean()\n",
        "        neg_loss = torch.log(torch.exp(neg_sim).sum())\n",
        "\n",
        "        return pos_loss + neg_loss\n",
        "\n",
        "def create_model_and_dataloader(data, batch_size=32):\n",
        "    # Create dataset\n",
        "    dataset = SmartWatchDataset(data, window_size=60, stride=10)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    # Feature dimension is features + time\n",
        "    input_dim = len(dataset.features) + 1  # +1 for time information\n",
        "\n",
        "    # Create model\n",
        "    model = SmartWatchTransformer(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=128,\n",
        "        nhead=8,\n",
        "        num_layers=4,\n",
        "        dropout=0.1,\n",
        "        output_dim=200  # 200-dim embedding as specified\n",
        "    )\n",
        "\n",
        "    # Create contrastive loss\n",
        "    contrastive_loss = TemporalContrastiveLoss(temperature=0.5)\n",
        "\n",
        "    # Move to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    contrastive_loss = contrastive_loss.to(device)\n",
        "\n",
        "    return model, dataloader, device, contrastive_loss\n",
        "\n",
        "model, dataloader, device, contrastive_loss = create_model_and_dataloader(data)\n",
        "\n",
        "print(f\"Model created with device: {device}\")\n",
        "print(f\"Dataloader created with {len(dataloader)} batches\")"
      ],
      "metadata": {
        "id": "RSrp9x3w_2k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from datetime import timedelta\n",
        "import pickle\n",
        "\n",
        "# Load embeddings and original data\n",
        "embedding_df = pd.read_pickle('smartwatch_embeddings.pkl')\n",
        "data = pd.read_pickle('smartwatch_data.pkl')\n",
        "print(f\"Loaded {len(embedding_df)} embeddings\")\n",
        "\n",
        "class TemporalPhenotypeGraph:\n",
        "    def __init__(self, embedding_df, data, k_neighbors=5, max_time_diff=24):\n",
        "        \"\"\"\n",
        "        Construct the temporal phenotype graph\n",
        "\n",
        "        Args:\n",
        "            embedding_df: DataFrame with embeddings\n",
        "            data: Original smartwatch data\n",
        "            k_neighbors: Number of similarity neighbors\n",
        "            max_time_diff: Max time difference (hours) for similarity edges\n",
        "        \"\"\"\n",
        "        self.embedding_df = embedding_df\n",
        "        self.data = data\n",
        "        self.k_neighbors = k_neighbors\n",
        "        self.max_time_diff = max_time_diff\n",
        "\n",
        "        # Create graph\n",
        "        self.G = nx.DiGraph()\n",
        "\n",
        "        # Build graph\n",
        "        self._add_nodes()\n",
        "        self._add_sequential_edges()\n",
        "        self._add_similarity_edges()\n",
        "\n",
        "        print(f\"Graph constructed with {len(self.G.nodes)} nodes and {len(self.G.edges)} edges\")\n",
        "\n",
        "    def _add_nodes(self):\n",
        "        \"\"\"Add nodes to the graph with attributes\"\"\"\n",
        "        print(\"Adding nodes...\")\n",
        "        for _, row in tqdm(self.embedding_df.iterrows(), total=len(self.embedding_df)):\n",
        "            idx = row['index']\n",
        "\n",
        "            # Get original data at this index\n",
        "            orig_data = self.data.iloc[idx]\n",
        "\n",
        "            # Create node attributes\n",
        "            attrs = {\n",
        "                'timestamp': row['timestamp'],\n",
        "                'embedding': row['embedding'],\n",
        "                'hr': orig_data['hr'],\n",
        "                'spo2': orig_data['spo2'],\n",
        "                'activity_level': orig_data['activity_level'],\n",
        "                'sleep_stage': orig_data['sleep_stage'],\n",
        "                'stress': orig_data['stress'],\n",
        "                'hrv': orig_data['hrv'],\n",
        "                # Add more attributes as needed\n",
        "            }\n",
        "\n",
        "            # Add node\n",
        "            self.G.add_node(idx, **attrs)\n",
        "\n",
        "    def _add_sequential_edges(self):\n",
        "        \"\"\"Add edges between consecutive time points\"\"\"\n",
        "        print(\"Adding sequential edges...\")\n",
        "        # Sort by timestamp\n",
        "        sorted_nodes = sorted(self.G.nodes(), key=lambda n: self.G.nodes[n]['timestamp'])\n",
        "\n",
        "        # Add edges between consecutive nodes\n",
        "        for i in range(len(sorted_nodes) - 1):\n",
        "            self.G.add_edge(\n",
        "                sorted_nodes[i],\n",
        "                sorted_nodes[i+1],\n",
        "                edge_type='sequential',\n",
        "                weight=1.0\n",
        "            )\n",
        "\n",
        "    def _add_similarity_edges(self):\n",
        "        \"\"\"Add edges between similar nodes\"\"\"\n",
        "        print(\"Adding similarity edges...\")\n",
        "        # Get all embeddings and indices\n",
        "        indices = list(self.G.nodes())\n",
        "        embeddings = np.array([self.G.nodes[idx]['embedding'] for idx in indices])\n",
        "        timestamps = np.array([self.G.nodes[idx]['timestamp'] for idx in indices])\n",
        "\n",
        "        # Compute all pairwise similarities\n",
        "        similarities = cosine_similarity(embeddings)\n",
        "\n",
        "        # Add similarity edges for each node\n",
        "        for i, idx in enumerate(tqdm(indices)):\n",
        "            # Get k nearest neighbors by similarity\n",
        "            neighbor_similarities = similarities[i]\n",
        "\n",
        "            # Sort neighbors by similarity (excluding self)\n",
        "            neighbor_indices = np.argsort(neighbor_similarities)[::-1][1:self.k_neighbors+1]\n",
        "\n",
        "            # Add edges\n",
        "            for j in neighbor_indices:\n",
        "                neighbor_idx = indices[j]\n",
        "                similarity = neighbor_similarities[j]\n",
        "\n",
        "                # Check time difference constraint\n",
        "                time_diff = abs((timestamps[i] - timestamps[j]).total_seconds() / 3600)\n",
        "                if time_diff <= self.max_time_diff:\n",
        "                    self.G.add_edge(\n",
        "                        idx,\n",
        "                        neighbor_idx,\n",
        "                        edge_type='similarity',\n",
        "                        weight=similarity,\n",
        "                        time_diff=time_diff\n",
        "                    )\n",
        "\n",
        "    def plot_graph_sample(self, n_nodes=100, figsize=(12, 12)):\n",
        "        \"\"\"Plot a sample of the graph\"\"\"\n",
        "        # Select a sample of nodes\n",
        "        sample_nodes = list(self.G.nodes())[:n_nodes]\n",
        "        G_sample = self.G.subgraph(sample_nodes)\n",
        "\n",
        "        # Create positions\n",
        "        pos = {}\n",
        "        for node in G_sample.nodes():\n",
        "            # Use timestamp for x-axis\n",
        "            timestamp = G_sample.nodes[node]['timestamp']\n",
        "            timestamp_float = timestamp.timestamp()\n",
        "\n",
        "            # Use activity level for y-axis\n",
        "            activity = G_sample.nodes[node]['activity_level']\n",
        "\n",
        "            pos[node] = (timestamp_float, activity)\n",
        "\n",
        "        # Create edge colors based on type\n",
        "        edge_colors = []\n",
        "        widths = []\n",
        "        for u, v, data in G_sample.edges(data=True):\n",
        "            if data['edge_type'] == 'sequential':\n",
        "                edge_colors.append('blue')\n",
        "                widths.append(1)\n",
        "            else:  # similarity\n",
        "                edge_colors.append('red')\n",
        "                widths.append(0.5)\n",
        "\n",
        "        # Node colors based on sleep stage\n",
        "        node_colors = []\n",
        "        for node in G_sample.nodes():\n",
        "            sleep = G_sample.nodes[node]['sleep_stage']\n",
        "            if sleep == 0:\n",
        "                node_colors.append('lightyellow')  # Awake\n",
        "            elif sleep == 1:\n",
        "                node_colors.append('lightblue')  # Light sleep\n",
        "            elif sleep == 2:\n",
        "                node_colors.append('blue')  # Deep sleep\n",
        "            else:\n",
        "                node_colors.append('purple')  # REM\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=figsize)\n",
        "        nx.draw_networkx(\n",
        "            G_sample, pos=pos,\n",
        "            with_labels=False,\n",
        "            node_size=30,\n",
        "            node_color=node_colors,\n",
        "            edge_color=edge_colors,\n",
        "            width=widths,\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "        plt.title('Temporal Phenotype Graph (Sample)')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Activity Level')\n",
        "\n",
        "        # Legend\n",
        "        plt.plot([0], [0], color='blue', label='Sequential Edge')\n",
        "        plt.plot([0], [0], color='red', label='Similarity Edge')\n",
        "        plt.plot([0], [0], marker='o', color='lightyellow', label='Awake', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='lightblue', label='Light Sleep', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='blue', label='Deep Sleep', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='purple', label='REM Sleep', linestyle='')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_graph(self, filename='temporal_phenotype_graph.pkl'):\n",
        "        \"\"\"Save the graph to a file\"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.G, f)\n",
        "        print(f\"Graph saved to '{filename}'\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_graph(cls, filename='temporal_phenotype_graph.pkl'):\n",
        "        \"\"\"Load a graph from file\"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            G = pickle.load(f)\n",
        "\n",
        "        # Create a new instance\n",
        "        instance = cls.__new__(cls)\n",
        "        instance.G = G\n",
        "\n",
        "        print(f\"Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges\")\n",
        "        return instance\n",
        "\n",
        "# Create the graph\n",
        "graph_builder = TemporalPhenotypeGraph(embedding_df, data, k_neighbors=5, max_time_diff=24)\n",
        "\n",
        "# Plot a sample of the graph\n",
        "graph_builder.plot_graph_sample(n_nodes=100)\n",
        "\n",
        "# Save the graph\n",
        "graph_builder.save_graph()"
      ],
      "metadata": {
        "id": "c1itmWh7_5V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from datetime import timedelta\n",
        "import random\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Load graph and data\n",
        "with open('temporal_phenotype_graph.pkl', 'rb') as f:\n",
        "    G = pickle.load(f)\n",
        "data = pd.read_pickle('smartwatch_data.pkl')\n",
        "user_meta = pd.read_json('user_meta.json')\n",
        "\n",
        "print(f\"Loaded graph with {len(G.nodes)} nodes and {len(G.edges)} edges\")\n",
        "\n",
        "class PhysiologicalSimulator:\n",
        "    def __init__(self, graph, data, user_meta):\n",
        "        \"\"\"\n",
        "        Simple physiological simulator for digital twin augmentation\n",
        "\n",
        "        Args:\n",
        "            graph: NetworkX graph\n",
        "            data: Original data\n",
        "            user_meta: User metadata\n",
        "        \"\"\"\n",
        "        self.G = graph\n",
        "        self.data = data\n",
        "        self.user_meta = user_meta\n",
        "\n",
        "        # Get baseline physiological values\n",
        "        self.baseline_hr = np.median(data['hr'])\n",
        "        self.baseline_spo2 = np.median(data['spo2'])\n",
        "        self.baseline_hrv = np.median(data['hrv'])\n",
        "\n",
        "        # Adjust baselines based on user metadata\n",
        "        self._adjust_baselines()\n",
        "\n",
        "        print(f\"Initialized physiological simulator with baselines:\")\n",
        "        print(f\"  HR: {self.baseline_hr:.1f} bpm\")\n",
        "        print(f\"  SpO2: {self.baseline_spo2:.1f} %\")\n",
        "        print(f\"  HRV: {self.baseline_hrv:.1f} ms\")\n",
        "\n",
        "    def _adjust_baselines(self):\n",
        "        \"\"\"Adjust baseline values based on user metadata\"\"\"\n",
        "        # Age effect on HR and HRV\n",
        "        age = self.user_meta['age']\n",
        "        if age > 50:\n",
        "            # Older people tend to have lower HRV\n",
        "            self.baseline_hrv *= 0.9\n",
        "\n",
        "        # BMI effect on HR and SpO2\n",
        "        bmi = self.user_meta['bmi']\n",
        "        if bmi > 30:  # Obese\n",
        "            # Higher HR and slightly lower SpO2 baseline\n",
        "            self.baseline_hr *= 1.05\n",
        "            self.baseline_spo2 *= 0.98\n",
        "\n",
        "        # Pre-existing conditions\n",
        "        conditions = self.user_meta.get('conditions', [])\n",
        "        if 'hypertension' in conditions:\n",
        "            self.baseline_hr *= 1.1\n",
        "        if 'asthma' in conditions:\n",
        "            self.baseline_spo2 *= 0.97\n",
        "\n",
        "    def simulate_exercise(self, duration_minutes=30, intensity=0.8):\n",
        "        \"\"\"\n",
        "        Simulate exercise response\n",
        "\n",
        "        Args:\n",
        "            duration_minutes: Duration of exercise in minutes\n",
        "            intensity: Exercise intensity (0-1)\n",
        "\n",
        "        Returns:\n",
        "            List of simulated nodes\n",
        "        \"\"\"\n",
        "        print(f\"Simulating exercise: {duration_minutes} minutes, intensity {intensity}\")\n",
        "\n",
        "        # Start with a random timestamp from awake periods\n",
        "        awake_nodes = [n for n, data in self.G.nodes(data=True)\n",
        "                      if data.get('sleep_stage', 0) == 0]\n",
        "        start_node = random.choice(awake_nodes)\n",
        "        start_time = self.G.nodes[start_node]['timestamp']\n",
        "\n",
        "        # Generate timestamps for the exercise\n",
        "        timestamps = [start_time + timedelta(minutes=i) for i in range(duration_minutes)]\n",
        "\n",
        "        # Generate physiological values\n",
        "        hr_values = []\n",
        "        spo2_values = []\n",
        "        hrv_values = []\n",
        "\n",
        "        # Exercise response curves\n",
        "        # HR increases quickly, plateaus, then decreases\n",
        "        # SpO2 decreases slightly during exercise\n",
        "        # HRV decreases during exercise\n",
        "\n",
        "        max_hr = self.baseline_hr * (1 + 0.7 * intensity)\n",
        "        min_spo2 = self.baseline_spo2 * (1 - 0.03 * intensity)\n",
        "        min_hrv = self.baseline_hrv * (1 - 0.5 * intensity)\n",
        "\n",
        "        # Time points for interpolation\n",
        "        t_points = np.linspace(0, 1, 5)  # 5 key points in normalized time\n",
        "\n",
        "        # HR curve: baseline -> quick rise -> plateau -> decline -> baseline\n",
        "        hr_curve = [\n",
        "            self.baseline_hr,\n",
        "            self.baseline_hr + 0.7 * (max_hr - self.baseline_hr),\n",
        "            max_hr,\n",
        "            self.baseline_hr + 0.3 * (max_hr - self.baseline_hr),\n",
        "            self.baseline_hr\n",
        "        ]\n",
        "        hr_interp = interp1d(t_points, hr_curve, kind='quadratic')\n",
        "\n",
        "        # SpO2 curve: baseline -> decline -> plateau -> recovery -> baseline\n",
        "        spo2_curve = [\n",
        "            self.baseline_spo2,\n",
        "            self.baseline_spo2 - 0.5 * (self.baseline_spo2 - min_spo2),\n",
        "            min_spo2,\n",
        "            self.baseline_spo2 - 0.2 * (self.baseline_spo2 - min_spo2),\n",
        "            self.baseline_spo2\n",
        "        ]\n",
        "        spo2_interp = interp1d(t_points, spo2_curve, kind='quadratic')\n",
        "\n",
        "        # HRV curve: baseline -> decline -> plateau -> recovery -> baseline\n",
        "        hrv_curve = [\n",
        "            self.baseline_hrv,\n",
        "            self.baseline_hrv - 0.7 * (self.baseline_hrv - min_hrv),\n",
        "            min_hrv,\n",
        "            self.baseline_hrv - 0.4 * (self.baseline_hrv - min_hrv),\n",
        "            self.baseline_hrv\n",
        "        ]\n",
        "        hrv_interp = interp1d(t_points, hrv_curve, kind='quadratic')\n",
        "\n",
        "        # Generate values\n",
        "        t_values = np.linspace(0, 1, duration_minutes)\n",
        "        hr_values = hr_interp(t_values) + np.random.normal(0, 2, duration_minutes)\n",
        "        spo2_values = spo2_interp(t_values) + np.random.normal(0, 0.5, duration_minutes)\n",
        "        hrv_values = hrv_interp(t_values) + np.random.normal(0, 2, duration_minutes)\n",
        "\n",
        "        # Ensure physiological ranges\n",
        "        spo2_values = np.clip(spo2_values, 85, 100)\n",
        "        hr_values = np.clip(hr_values, 40, 200)\n",
        "        hrv_values = np.clip(hrv_values, 5, 100)\n",
        "\n",
        "        # Create simulated nodes\n",
        "        simulated_nodes = []\n",
        "\n",
        "        # Get a sample embedding to use as template\n",
        "        template_embedding = np.mean([data['embedding'] for _, data in\n",
        "                                      random.sample(list(self.G.nodes(data=True)), 10)], axis=0)\n",
        "\n",
        "        for i in range(duration_minutes):\n",
        "            # Perturb the embedding based on exercise state\n",
        "            perturbed_embedding = template_embedding + 0.1 * np.random.randn(len(template_embedding))\n",
        "\n",
        "            # Higher activity during exercise\n",
        "            activity = 0.7 + 0.3 * intensity + 0.1 * np.random.random()\n",
        "\n",
        "            # Create node ID - use negative numbers for virtual nodes to avoid collision\n",
        "            node_id = -1 - i\n",
        "\n",
        "            # Node attributes\n",
        "            attrs = {\n",
        "                'timestamp': timestamps[i],\n",
        "                'embedding': perturbed_embedding,\n",
        "                'hr': hr_values[i],\n",
        "                'spo2': spo2_values[i],\n",
        "                'activity_level': activity,\n",
        "                'sleep_stage': 0,  # Awake\n",
        "                'stress': 50 + 20 * intensity + np.random.normal(0, 5),  # Higher stress during exercise\n",
        "                'hrv': hrv_values[i],\n",
        "                'is_virtual': True,  # Mark as virtual node\n",
        "                'virtual_type': 'exercise'\n",
        "            }\n",
        "\n",
        "            # Add to list\n",
        "            simulated_nodes.append((node_id, attrs))\n",
        "\n",
        "        return simulated_nodes\n",
        "\n",
        "    def simulate_hypoxia(self, duration_minutes=60, severity=0.5):\n",
        "        \"\"\"\n",
        "        Simulate hypoxic events (e.g., sleep apnea episodes)\n",
        "\n",
        "        Args:\n",
        "            duration_minutes: Duration of event in minutes\n",
        "            severity: Severity of hypoxia (0-1)\n",
        "\n",
        "        Returns:\n",
        "            List of simulated nodes\n",
        "        \"\"\"\n",
        "        print(f\"Simulating hypoxia: {duration_minutes} minutes, severity {severity}\")\n",
        "\n",
        "        # Start with a random timestamp from sleep periods\n",
        "        sleep_nodes = [n for n, data in self.G.nodes(data=True)\n",
        "                      if data.get('sleep_stage', 0) > 0]\n",
        "        if not sleep_nodes:\n",
        "            sleep_nodes = list(self.G.nodes())\n",
        "\n",
        "        start_node = random.choice(sleep_nodes)\n",
        "        start_time = self.G.nodes[start_node]['timestamp']\n",
        "\n",
        "        # Generate timestamps for the event\n",
        "        timestamps = [start_time + timedelta(minutes=i) for i in range(duration_minutes)]\n",
        "\n",
        "        # Hypoxia parameters\n",
        "        min_spo2 = self.baseline_spo2 - 15 * severity  # Can drop to 85% for mild, 70% for severe\n",
        "        max_hr = self.baseline_hr + 20 * severity  # HR increases during hypoxic events\n",
        "        min_hrv = self.baseline_hrv - 15 * severity  # HRV decreases\n",
        "\n",
        "        # Time points for interpolation\n",
        "        t_points = np.linspace(0, 1, 5)  # 5 key points in normalized time\n",
        "\n",
        "        # SpO2 curve: baseline -> rapid drop -> plateau -> recovery -> baseline\n",
        "        spo2_curve = [\n",
        "            self.baseline_spo2,\n",
        "            self.baseline_spo2 - 0.7 * (self.baseline_spo2 - min_spo2),\n",
        "            min_spo2,\n",
        "            self.baseline_spo2 - 0.3 * (self.baseline_spo2 - min_spo2),\n",
        "            self.baseline_spo2\n",
        "        ]\n",
        "        spo2_interp = interp1d(t_points, spo2_curve, kind='quadratic')\n",
        "\n",
        "        # HR curve: baseline -> increase -> plateau -> decline -> baseline\n",
        "        hr_curve = [\n",
        "            self.baseline_hr,\n",
        "            self.baseline_hr + 0.5 * (max_hr - self.baseline_hr),\n",
        "            max_hr,\n",
        "            self.baseline_hr + 0.3 * (max_hr - self.baseline_hr),\n",
        "            self.baseline_hr\n",
        "        ]\n",
        "        hr_interp = interp1d(t_points, hr_curve, kind='quadratic')\n",
        "\n",
        "        # HRV curve: baseline -> decrease -> plateau -> recovery -> baseline\n",
        "        hrv_curve = [\n",
        "            self.baseline_hrv,\n",
        "            self.baseline_hrv - 0.6 * (self.baseline_hrv - min_hrv),\n",
        "            min_hrv,\n",
        "            self.baseline_hrv - 0.3 * (self.baseline_hrv - min_hrv),\n",
        "            self.baseline_hrv\n",
        "        ]\n",
        "        hrv_interp = interp1d(t_points, hrv_curve, kind='quadratic')\n",
        "\n",
        "        # Generate values with timing offsets (HR changes lag SpO2)\n",
        "        t_values = np.linspace(0, 1, duration_minutes)\n",
        "        spo2_values = spo2_interp(t_values) + np.random.normal(0, 0.7, duration_minutes)\n",
        "\n",
        "        # HR response lags behind SpO2 drop\n",
        "        t_hr = np.clip(t_values - 0.05, 0, 1)  # Shift by 5% of the time\n",
        "        hr_values = hr_interp(t_hr) + np.random.normal(0, 3, duration_minutes)\n",
        "\n",
        "        # HRV response also lags\n",
        "        t_hrv = np.clip(t_values - 0.03, 0, 1)  # Shift by 3% of the time\n",
        "        hrv_values = hrv_interp(t_hrv) + np.random.normal(0, 2, duration_minutes)\n",
        "\n",
        "        # Ensure physiological ranges\n",
        "        spo2_values = np.clip(spo2_values, 70, 100)\n",
        "        hr_values = np.clip(hr_values, 40, 200)\n",
        "        hrv_values = np.clip(hrv_values, 5, 100)\n",
        "\n",
        "        # Create simulated nodes\n",
        "        simulated_nodes = []\n",
        "\n",
        "        # Get a sample embedding to use as template (from sleep states)\n",
        "        sleep_embeddings = [data['embedding'] for _, data in\n",
        "                           filter(lambda x: x[1].get('sleep_stage', 0) > 0,\n",
        "                                 self.G.nodes(data=True))]\n",
        "\n",
        "        if sleep_embeddings:\n",
        "            template_embedding = np.mean(random.sample(sleep_embeddings,\n",
        "                                                      min(10, len(sleep_embeddings))), axis=0)\n",
        "        else:\n",
        "            # Fallback to any nodes\n",
        "            template_embedding = np.mean([data['embedding'] for _, data in\n",
        "                                          random.sample(list(self.G.nodes(data=True)), 10)], axis=0)\n",
        "\n",
        "        for i in range(duration_minutes):\n",
        "            # Perturb the embedding based on hypoxia state\n",
        "            perturbed_embedding = template_embedding + 0.1 * np.random.randn(len(template_embedding))\n",
        "\n",
        "            # Sleep stage - mostly deep or REM\n",
        "            sleep_stage = np.random.choice([2, 3], p=[0.3, 0.7])\n",
        "\n",
        "            # Create node ID - use negative numbers for virtual nodes\n",
        "            node_id = -1000 - i\n",
        "\n",
        "            # Node attributes\n",
        "            attrs = {\n",
        "                'timestamp': timestamps[i],\n",
        "                'embedding': perturbed_embedding,\n",
        "                'hr': hr_values[i],\n",
        "                'spo2': spo2_values[i],\n",
        "                'activity_level': 0.05 + 0.05 * np.random.random(),  # Very low during sleep\n",
        "                'sleep_stage': sleep_stage,\n",
        "                'stress': 30 + 30 * severity + np.random.normal(0, 5),  # Stress increases during hypoxia\n",
        "                'hrv': hrv_values[i],\n",
        "                'is_virtual': True,  # Mark as virtual node\n",
        "                'virtual_type': 'hypoxia'\n",
        "            }\n",
        "\n",
        "            # Add to list\n",
        "            simulated_nodes.append((node_id, attrs))\n",
        "\n",
        "        return simulated_nodes\n",
        "\n",
        "    def augment_graph(self, num_exercise=3, num_hypoxia=2):\n",
        "        \"\"\"\n",
        "        Augment the graph with simulated nodes and edges\n",
        "\n",
        "        Args:\n",
        "            num_exercise: Number of exercise events to simulate\n",
        "            num_hypoxia: Number of hypoxic events to simulate\n",
        "\n",
        "        Returns:\n",
        "            Augmented graph\n",
        "        \"\"\"\n",
        "        G_aug = self.G.copy()\n",
        "        added_nodes = 0\n",
        "\n",
        "        # Add exercise events\n",
        "        for i in range(num_exercise):\n",
        "            # Random duration between 20-45 minutes\n",
        "            duration = np.random.randint(20, 46)\n",
        "            # Random intensity between 0.6-0.9\n",
        "            intensity = 0.6 + 0.3 * np.random.random()\n",
        "\n",
        "            # Generate virtual nodes\n",
        "            exercise_nodes = self.simulate_exercise(duration, intensity)\n",
        "\n",
        "            # Add to graph\n",
        "            for node_id, attrs in exercise_nodes:\n",
        "                G_aug.add_node(node_id, **attrs)\n",
        "                added_nodes += 1\n",
        "\n",
        "            # Connect sequentially\n",
        "            for i in range(len(exercise_nodes) - 1):\n",
        "                G_aug.add_edge(\n",
        "                    exercise_nodes[i][0],\n",
        "                    exercise_nodes[i+1][0],\n",
        "                    edge_type='sequential',\n",
        "                    weight=1.0,\n",
        "                    is_virtual=True\n",
        "                )\n",
        "\n",
        "        # Add hypoxic events\n",
        "        for i in range(num_hypoxia):\n",
        "            # Random duration between 15-90 minutes\n",
        "            duration = np.random.randint(15, 91)\n",
        "            # Random severity between 0.3-0.8\n",
        "            severity = 0.3 + 0.5 * np.random.random()\n",
        "\n",
        "            # Generate virtual nodes\n",
        "            hypoxia_nodes = self.simulate_hypoxia(duration, severity)\n",
        "\n",
        "            # Add to graph\n",
        "            for node_id, attrs in hypoxia_nodes:\n",
        "                G_aug.add_node(node_id, **attrs)\n",
        "                added_nodes += 1\n",
        "\n",
        "            # Connect sequentially\n",
        "            for i in range(len(hypoxia_nodes) - 1):\n",
        "                G_aug.add_edge(\n",
        "                    hypoxia_nodes[i][0],\n",
        "                    hypoxia_nodes[i+1][0],\n",
        "                    edge_type='sequential',\n",
        "                    weight=1.0,\n",
        "                    is_virtual=True\n",
        "                )\n",
        "\n",
        "        print(f\"Added {added_nodes} virtual nodes to the graph\")\n",
        "\n",
        "        # Add similarity edges between virtual and real nodes\n",
        "        self._connect_virtual_nodes(G_aug)\n",
        "\n",
        "        return G_aug\n",
        "\n",
        "    def _connect_virtual_nodes(self, G_aug, k=3):\n",
        "        \"\"\"\n",
        "        Connect virtual nodes to similar real nodes\n",
        "\n",
        "        Args:\n",
        "            G_aug: Augmented graph\n",
        "            k: Number of connections per virtual node\n",
        "        \"\"\"\n",
        "        print(\"Connecting virtual nodes to similar real nodes...\")\n",
        "\n",
        "        # Get all real and virtual nodes\n",
        "        real_nodes = [(n, data['embedding']) for n, data in G_aug.nodes(data=True)\n",
        "                     if not data.get('is_virtual', False)]\n",
        "        virtual_nodes = [(n, data['embedding']) for n, data in G_aug.nodes(data=True)\n",
        "                        if data.get('is_virtual', False)]\n",
        "\n",
        "        if not real_nodes or not virtual_nodes:\n",
        "            return\n",
        "\n",
        "        # Convert to arrays for faster computation\n",
        "        real_indices = [n[0] for n in real_nodes]\n",
        "        real_embeddings = np.array([n[1] for n in real_nodes])\n",
        "\n",
        "        # For each virtual node, find k similar real nodes\n",
        "        for v_node, v_embedding in tqdm(virtual_nodes):\n",
        "            # Compute similarities\n",
        "            similarities = np.dot(real_embeddings, v_embedding) / (\n",
        "                np.linalg.norm(real_embeddings, axis=1) * np.linalg.norm(v_embedding)\n",
        "            )\n",
        "\n",
        "            # Get top k similar nodes\n",
        "            top_k_indices = np.argsort(similarities)[-k:]\n",
        "\n",
        "            # Add edges\n",
        "            for idx in top_k_indices:\n",
        "                real_node = real_indices[idx]\n",
        "                similarity = similarities[idx]\n",
        "\n",
        "                # Add bidirectional similarity edges\n",
        "                G_aug.add_edge(\n",
        "                    v_node,\n",
        "                    real_node,\n",
        "                    edge_type='similarity',\n",
        "                    weight=similarity,\n",
        "                    is_virtual=True\n",
        "                )\n",
        "                G_aug.add_edge(\n",
        "                    real_node,\n",
        "                    v_node,\n",
        "                    edge_type='similarity',\n",
        "                    weight=similarity,\n",
        "                    is_virtual=True\n",
        "                )\n",
        "\n",
        "    def plot_augmented_graph_sample(self, G_aug, n_nodes=150, figsize=(14, 10)):\n",
        "        \"\"\"\n",
        "        Plot a sample of the augmented graph\n",
        "\n",
        "        Args:\n",
        "            G_aug: Augmented graph\n",
        "            n_nodes: Number of nodes to show\n",
        "            figsize: Figure size\n",
        "        \"\"\"\n",
        "        # Select a sample of nodes with at least some virtual nodes\n",
        "        virtual_nodes = [n for n, data in G_aug.nodes(data=True)\n",
        "                        if data.get('is_virtual', False)]\n",
        "        real_nodes = [n for n, data in G_aug.nodes(data=True)\n",
        "                     if not data.get('is_virtual', False)]\n",
        "\n",
        "        # Take all virtual nodes and some real nodes\n",
        "        selected_virtual = virtual_nodes[:min(len(virtual_nodes), n_nodes//3)]\n",
        "        selected_real = np.random.choice(real_nodes,\n",
        "                                        size=min(len(real_nodes), n_nodes - len(selected_virtual)),\n",
        "                                        replace=False)\n",
        "\n",
        "        sample_nodes = list(selected_virtual) + list(selected_real)\n",
        "        G_sample = G_aug.subgraph(sample_nodes)\n",
        "\n",
        "        # Create positions\n",
        "        pos = {}\n",
        "        for node in G_sample.nodes():\n",
        "            # Use timestamp for x-axis\n",
        "            timestamp = G_sample.nodes[node]['timestamp']\n",
        "            timestamp_float = timestamp.timestamp()\n",
        "\n",
        "            # Use activity level for y-axis\n",
        "            activity = G_sample.nodes[node]['activity_level']\n",
        "\n",
        "            pos[node] = (timestamp_float, activity)\n",
        "\n",
        "        # Create edge colors based on type\n",
        "        edge_colors = []\n",
        "        widths = []\n",
        "        for u, v, data in G_sample.edges(data=True):\n",
        "            if data.get('is_virtual', False):\n",
        "                if data['edge_type'] == 'sequential':\n",
        "                    edge_colors.append('purple')\n",
        "                    widths.append(1)\n",
        "                else:  # similarity\n",
        "                    edge_colors.append('orange')\n",
        "                    widths.append(0.5)\n",
        "            else:\n",
        "                if data['edge_type'] == 'sequential':\n",
        "                    edge_colors.append('blue')\n",
        "                    widths.append(1)\n",
        "                else:  # similarity\n",
        "                    edge_colors.append('red')\n",
        "                    widths.append(0.5)\n",
        "\n",
        "        # Node colors based on real/virtual status\n",
        "        node_colors = []\n",
        "        node_sizes = []\n",
        "        for node in G_sample.nodes():\n",
        "            data = G_sample.nodes[node]\n",
        "\n",
        "            if data.get('is_virtual', False):\n",
        "                if data.get('virtual_type') == 'exercise':\n",
        "                    node_colors.append('lime')  # Virtual exercise\n",
        "                else:\n",
        "                    node_colors.append('magenta')  # Virtual hypoxia\n",
        "                node_sizes.append(50)\n",
        "            else:\n",
        "                sleep = data['sleep_stage']\n",
        "                if sleep == 0:\n",
        "                    node_colors.append('lightyellow')  # Awake\n",
        "                elif sleep == 1:\n",
        "                    node_colors.append('lightblue')  # Light sleep\n",
        "                elif sleep == 2:\n",
        "                    node_colors.append('blue')  # Deep sleep\n",
        "                else:\n",
        "                    node_colors.append('purple')  # REM\n",
        "                node_sizes.append(30)\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=figsize)\n",
        "        nx.draw_networkx(\n",
        "            G_sample, pos=pos,\n",
        "            with_labels=False,\n",
        "            node_size=node_sizes,\n",
        "            node_color=node_colors,\n",
        "            edge_color=edge_colors,\n",
        "            width=widths,\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "        plt.title('Augmented Temporal Phenotype Graph (Sample)')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Activity Level')\n",
        "\n",
        "        # Legend\n",
        "        plt.plot([0], [0], color='blue', label='Real Sequential Edge')\n",
        "        plt.plot([0], [0], color='red', label='Real Similarity Edge')\n",
        "        plt.plot([0], [0], color='purple', label='Virtual Sequential Edge')\n",
        "        plt.plot([0], [0], color='orange', label='Virtual Similarity Edge')\n",
        "        plt.plot([0], [0], marker='o', color='lightyellow', label='Awake', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='lightblue', label='Light Sleep', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='blue', label='Deep Sleep', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='purple', label='REM Sleep', linestyle='')\n",
        "        plt.plot([0], [0], marker='o', color='lime', label='Virtual Exercise', linestyle='', markersize=10)\n",
        "        plt.plot([0], [0], marker='o', color='magenta', label='Virtual Hypoxia', linestyle='', markersize=10)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_augmented_graph(self, G_aug, filename='augmented_phenotype_graph.pkl'):\n",
        "        \"\"\"Save the augmented graph to a file\"\"\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(G_aug, f)\n",
        "        print(f\"Augmented graph saved to '{filename}'\")\n",
        "\n",
        "# Create the simulator\n",
        "simulator = PhysiologicalSimulator(G, data, user_meta)\n",
        "\n",
        "# Augment the graph\n",
        "G_aug = simulator.augment_graph(num_exercise=3, num_hypoxia=2)\n",
        "print(f\"Augmented graph has {len(G_aug.nodes)} nodes and {len(G_aug.edges)} edges\")\n",
        "\n",
        "# Plot the augmented graph\n",
        "simulator.plot_augmented_graph_sample(G_aug, n_nodes=150)\n",
        "\n",
        "# Save the augmented graph\n",
        "simulator.save_augmented_graph(G_aug)"
      ],
      "metadata": {
        "id": "79QP9Kfs_-yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
        "from torch_geometric.data import Data, Batch\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the augmented graph\n",
        "with open('augmented_phenotype_graph.pkl', 'rb') as f:\n",
        "    G_aug = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded augmented graph with {len(G_aug.nodes)} nodes and {len(G_aug.edges)} edges\")\n",
        "\n",
        "class GraphDatasetGenerator:\n",
        "    def __init__(self, G, window_size=60, stride=10):\n",
        "        \"\"\"\n",
        "        Create a dataset of graph windows from the temporal graph\n",
        "\n",
        "        Args:\n",
        "            G: NetworkX graph (temporal phenotype graph)\n",
        "            window_size: Window size in nodes\n",
        "            stride: Stride for window creation\n",
        "        \"\"\"\n",
        "        self.G = G\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # Get sorted nodes by timestamp\n",
        "        self.sorted_nodes = sorted(\n",
        "            list(G.nodes()),\n",
        "            key=lambda n: G.nodes[n]['timestamp']\n",
        "        )\n",
        "\n",
        "        # Create windows\n",
        "        self.windows = []\n",
        "        for i in range(0, len(self.sorted_nodes) - window_size, stride):\n",
        "            self.windows.append(self.sorted_nodes[i:i+window_size])\n",
        "\n",
        "        print(f\"Created {len(self.windows)} graph windows\")\n",
        "\n",
        "        # Node feature dimension\n",
        "        sample_node = self.sorted_nodes[0]\n",
        "        sample_embedding = G.nodes[sample_node]['embedding']\n",
        "        self.embedding_dim = len(sample_embedding)\n",
        "\n",
        "        # Define node features to extract\n",
        "        self.node_features = [\n",
        "            'hr', 'spo2', 'activity_level', 'sleep_stage',\n",
        "            'stress', 'hrv'\n",
        "        ]\n",
        "\n",
        "    def get_window_data(self, window_idx):\n",
        "        \"\"\"\n",
        "        Extract a PyTorch Geometric Data object for a window\n",
        "\n",
        "        Args:\n",
        "            window_idx: Index of the window\n",
        "\n",
        "        Returns:\n",
        "            PyTorch Geometric Data object\n",
        "        \"\"\"\n",
        "        # Get nodes in this window\n",
        "        window_nodes = self.windows[window_idx]\n",
        "\n",
        "        # Extract subgraph\n",
        "        subgraph = self.G.subgraph(window_nodes)\n",
        "\n",
        "        # Create node mapping (for edge indices)\n",
        "        node_mapping = {node: i for i, node in enumerate(window_nodes)}\n",
        "\n",
        "        # Get node features\n",
        "        node_embeddings = []\n",
        "        node_features = []\n",
        "\n",
        "        for node in window_nodes:\n",
        "            # Get embedding\n",
        "            embedding = self.G.nodes[node]['embedding']\n",
        "            node_embeddings.append(embedding)\n",
        "\n",
        "            # Get other features\n",
        "            features = [self.G.nodes[node].get(feat, 0) for feat in self.node_features]\n",
        "            node_features.append(features)\n",
        "\n",
        "        # Combine embeddings and features\n",
        "        node_embeddings = torch.tensor(np.array(node_embeddings), dtype=torch.float)\n",
        "        node_features = torch.tensor(np.array(node_features), dtype=torch.float)\n",
        "\n",
        "        # Normalize node features\n",
        "        node_features = (node_features - node_features.mean(dim=0)) / (node_features.std(dim=0) + 1e-6)\n",
        "\n",
        "        # Combine all node features\n",
        "        x = torch.cat([node_embeddings, node_features], dim=1)\n",
        "\n",
        "        # Get edges\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "\n",
        "        for u, v, data in subgraph.edges(data=True):\n",
        "            # Convert node IDs to local indices\n",
        "            src_idx = node_mapping[u]\n",
        "            dst_idx = node_mapping[v]\n",
        "\n",
        "            edge_index.append([src_idx, dst_idx])\n",
        "\n",
        "            # Edge attributes\n",
        "            edge_type = 1.0 if data.get('edge_type') == 'sequential' else 0.0\n",
        "            is_virtual = 1.0 if data.get('is_virtual', False) else 0.0\n",
        "            weight = data.get('weight', 1.0)\n",
        "\n",
        "            edge_attr.append([edge_type, is_virtual, weight])\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "        # Create Data object\n",
        "        data = Data(\n",
        "            x=x,\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=edge_attr,\n",
        "            num_nodes=len(window_nodes)\n",
        "        )\n",
        "\n",
        "        return data\n",
        "\n",
        "    def generate_afib_labels(self):\n",
        "        \"\"\"\n",
        "        Generate synthetic AFib labels for windows\n",
        "\n",
        "        Returns:\n",
        "            Tensor of binary labels for each window\n",
        "        \"\"\"\n",
        "        # For synthetic data, we'll define AFib as windows containing:\n",
        "        # 1. HR variability (rapid changes)\n",
        "        # 2. Low HRV\n",
        "        # 3. Some virtual hypoxia nodes\n",
        "\n",
        "        labels = []\n",
        "\n",
        "        for window_idx in tqdm(range(len(self.windows)), desc=\"Generating AFib labels\"):\n",
        "            window_nodes = self.windows[window_idx]\n",
        "\n",
        "            # Check for HR variability\n",
        "            hrs = [self.G.nodes[n]['hr'] for n in window_nodes]\n",
        "            hr_std = np.std(hrs)\n",
        "\n",
        "            # Check for low HRV\n",
        "            hrvs = [self.G.nodes[n]['hrv'] for n in window_nodes]\n",
        "            hrv_mean = np.mean(hrvs)\n",
        "\n",
        "            # Check for virtual hypoxia nodes\n",
        "            has_hypoxia = any(\n",
        "                self.G.nodes[n].get('virtual_type') == 'hypoxia'\n",
        "                for n in window_nodes\n",
        "                if self.G.nodes[n].get('is_virtual', False)\n",
        "            )\n",
        "\n",
        "            # Combine criteria\n",
        "            is_afib = (hr_std > 15) and (hrv_mean < 30) and has_hypoxia\n",
        "\n",
        "            labels.append(1.0 if is_afib else 0.0)\n",
        "\n",
        "        return torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def generate_diabetes_labels(self):\n",
        "        \"\"\"\n",
        "        Generate synthetic diabetes risk labels for windows\n",
        "\n",
        "        Returns:\n",
        "            Tensor of binary labels for each window\n",
        "        \"\"\"\n",
        "        # For synthetic data, we'll define diabetes risk as windows containing:\n",
        "        # 1. High resting heart rate\n",
        "        # 2. Low activity levels\n",
        "        # 3. High stress levels\n",
        "\n",
        "        labels = []\n",
        "\n",
        "        for window_idx in tqdm(range(len(self.windows)), desc=\"Generating diabetes labels\"):\n",
        "            window_nodes = self.windows[window_idx]\n",
        "\n",
        "            # Get resting HR (during low activity)\n",
        "            hr_rest = [\n",
        "                self.G.nodes[n]['hr']\n",
        "                for n in window_nodes\n",
        "                if self.G.nodes[n]['activity_level'] < 0.2\n",
        "            ]\n",
        "\n",
        "            if not hr_rest:  # No resting periods in this window\n",
        "                labels.append(0.0)\n",
        "                continue\n",
        "\n",
        "            mean_rest_hr = np.mean(hr_rest)\n",
        "\n",
        "            # Get mean activity level\n",
        "            mean_activity = np.mean([self.G.nodes[n]['activity_level'] for n in window_nodes])\n",
        "\n",
        "            # Get mean stress level\n",
        "            mean_stress = np.mean([self.G.nodes[n]['stress'] for n in window_nodes])\n",
        "\n",
        "            # Combine criteria\n",
        "            risk_score = (mean_rest_hr > 75) + (mean_activity < 0.25) + (mean_stress > 60)\n",
        "            is_diabetes_risk = risk_score >= 2  # At least 2 of 3 criteria\n",
        "\n",
        "            labels.append(1.0 if is_diabetes_risk else 0.0)\n",
        "\n",
        "        return torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def generate_copd_labels(self):\n",
        "        \"\"\"\n",
        "        Generate synthetic COPD risk labels for windows\n",
        "\n",
        "        Returns:\n",
        "            Tensor of binary labels for each window\n",
        "        \"\"\"\n",
        "        # For synthetic data, we'll define COPD risk as windows containing:\n",
        "        # 1. Low SpO2 levels\n",
        "        # 2. High heart rate with low activity\n",
        "        # 3. Presence of virtual hypoxia events\n",
        "\n",
        "        labels = []\n",
        "\n",
        "        for window_idx in tqdm(range(len(self.windows)), desc=\"Generating COPD labels\"):\n",
        "            window_nodes = self.windows[window_idx]\n",
        "\n",
        "            # Check for low SpO2\n",
        "            spo2_values = [self.G.nodes[n]['spo2'] for n in window_nodes]\n",
        "            min_spo2 = np.min(spo2_values)\n",
        "            mean_spo2 = np.mean(spo2_values)\n",
        "\n",
        "            # Check for high HR with low activity\n",
        "            hr_values = []\n",
        "            activity_values = []\n",
        "\n",
        "            for n in window_nodes:\n",
        "                hr = self.G.nodes[n]['hr']\n",
        "                activity = self.G.nodes[n]['activity_level']\n",
        "\n",
        "                if activity < 0.3:  # Only consider low activity periods\n",
        "                    hr_values.append(hr)\n",
        "                    activity_values.append(activity)\n",
        "\n",
        "            if not hr_values:  # No low activity periods\n",
        "                labels.append(0.0)\n",
        "                continue\n",
        "\n",
        "            mean_rest_hr = np.mean(hr_values)\n",
        "\n",
        "            # Check for virtual hypoxia nodes\n",
        "            has_hypoxia = any(\n",
        "                self.G.nodes[n].get('virtual_type') == 'hypoxia'\n",
        "                for n in window_nodes\n",
        "                if self.G.nodes[n].get('is_virtual', False)\n",
        "            )\n",
        "\n",
        "            # Combine criteria\n",
        "            is_copd_risk = (min_spo2 < 90) and (mean_spo2 < 95) and (mean_rest_hr > 80 or has_hypoxia)\n",
        "\n",
        "            labels.append(1.0 if is_copd_risk else 0.0)\n",
        "\n",
        "        return torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    def create_dataset(self, disease_type='afib'):\n",
        "        \"\"\"\n",
        "        Create a complete dataset for a disease type\n",
        "\n",
        "        Args:\n",
        "            disease_type: 'afib', 'diabetes', or 'copd'\n",
        "\n",
        "        Returns:\n",
        "            List of Data objects, labels\n",
        "        \"\"\"\n",
        "        # Generate labels based on disease type\n",
        "        if disease_type.lower() == 'afib':\n",
        "            labels = self.generate_afib_labels()\n",
        "        elif disease_type.lower() == 'diabetes':\n",
        "            labels = self.generate_diabetes_labels()\n",
        "        elif disease_type.lower() == 'copd':\n",
        "            labels = self.generate_copd_labels()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown disease type: {disease_type}\")\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = []\n",
        "        for i in tqdm(range(len(self.windows)), desc=f\"Creating {disease_type} dataset\"):\n",
        "            data = self.get_window_data(i)\n",
        "            data.y = labels[i]\n",
        "            dataset.append(data)\n",
        "\n",
        "        return dataset, labels\n",
        "\n",
        "class DiseaseMotifGNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_layers=3):\n",
        "        \"\"\"\n",
        "        GNN for detecting disease motifs in the graph\n",
        "\n",
        "        Args:\n",
        "            input_dim: Dimension of input node features\n",
        "            hidden_dim: Hidden dimension\n",
        "            num_layers: Number of GNN layers\n",
        "        \"\"\"\n",
        "        super(DiseaseMotifGNN, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Graph convolution layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            # Alternating GCN and GAT layers\n",
        "            if i % 2 == 0:\n",
        "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "            else:\n",
        "                self.convs.append(GATConv(hidden_dim, hidden_dim))\n",
        "\n",
        "        # Pooling projection\n",
        "        self.pool_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "        # Output layers\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc2 = nn.Linear(hidden_dim // 2, 1)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "        # Initial projection\n",
        "        x = self.input_proj(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Graph convolutions\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global pooling (combine mean and max)\n",
        "        x_mean = global_mean_pool(x, batch)\n",
        "        x_max = global_max_pool(x, batch)\n",
        "        x = torch.cat([x_mean, x_max], dim=1)\n",
        "\n",
        "        # Pooling projection\n",
        "        x = self.pool_proj(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Output layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return torch.sigmoid(x).squeeze()\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=50, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train a disease motif GNN model\n",
        "\n",
        "    Args:\n",
        "        model: Model to train\n",
        "        train_loader: Training data loader\n",
        "        val_loader: Validation data loader\n",
        "        num_epochs: Number of training epochs\n",
        "        lr: Learning rate\n",
        "\n",
        "    Returns:\n",
        "        Trained model, training history\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    # Early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch)\n",
        "            loss = criterion(outputs, batch.y)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            train_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = (outputs > 0.5).float()\n",
        "            train_correct += (preds == batch.y).sum().item()\n",
        "            train_total += batch.y.size(0)\n",
        "\n",
        "        train_loss /= train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                batch = batch.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(batch)\n",
        "                loss = criterion(outputs, batch.y)\n",
        "\n",
        "                # Update statistics\n",
        "                val_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "                # Calculate accuracy\n",
        "                preds = (outputs > 0.5).float()\n",
        "                val_correct += (preds == batch.y).sum().item()\n",
        "                val_total += batch.y.size(0)\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), f'best_model.pt')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    ax1.plot(history['train_loss'], label='Train Loss')\n",
        "    ax1.plot(history['val_loss'], label='Val Loss')\n",
        "    ax1.set_title('Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    ax2.plot(history['train_acc'], label='Train Acc')\n",
        "    ax2.plot(history['val_acc'], label='Val Acc')\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "# Create dataset generator\n",
        "dataset_gen = GraphDatasetGenerator(G_aug, window_size=60, stride=10)\n",
        "\n",
        "# Create datasets for different diseases\n",
        "afib_dataset, afib_labels = dataset_gen.create_dataset(disease_type='afib')\n",
        "diabetes_dataset, diabetes_labels = dataset_gen.create_dataset(disease_type='diabetes')\n",
        "copd_dataset, copd_labels = dataset_gen.create_dataset(disease_type='copd')\n",
        "\n",
        "print(f\"AFib dataset: {len(afib_dataset)} samples, {sum(afib_labels).item()} positive\")\n",
        "print(f\"Diabetes dataset: {len(diabetes_dataset)} samples, {sum(diabetes_labels).item()} positive\")\n",
        "print(f\"COPD dataset: {len(copd_dataset)} samples, {sum(copd_labels).item()} positive\")\n",
        "\n",
        "# Let's train a model for AFib detection as an example\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split dataset into train/val\n",
        "train_indices, val_indices = train_test_split(\n",
        "    range(len(afib_dataset)),\n",
        "    test_size=0.2,\n",
        "    stratify=afib_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = [afib_dataset[i] for i in train_indices]\n",
        "val_dataset = [afib_dataset[i] for i in val_indices]\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Create and train model\n",
        "input_dim = afib_dataset[0].x.shape[1]  # Node feature dimension\n",
        "model = DiseaseMotifGNN(input_dim=input_dim, hidden_dim=64, num_layers=3)\n",
        "\n",
        "trained_model, history = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=30,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Save trained model\n",
        "torch.save(trained_model.state_dict(), 'afib_gnn_model.pt')\n",
        "print(\"AFib GNN model saved to 'afib_gnn_model.pt'\")\n",
        "\n",
        "# Optionally train models for other diseases\n",
        "# For this example, we'll skip training the other models\n",
        "# But you could uncomment these sections to train them\n",
        "\n",
        "'''\n",
        "# Diabetes model\n",
        "diabetes_train_indices, diabetes_val_indices = train_test_split(\n",
        "    range(len(diabetes_dataset)),\n",
        "    test_size=0.2,\n",
        "    stratify=diabetes_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "diabetes_train_dataset = [diabetes_dataset[i] for i in diabetes_train_indices]\n",
        "diabetes_val_dataset = [diabetes_dataset[i] for i in diabetes_val_indices]\n",
        "\n",
        "diabetes_train_loader = DataLoader(diabetes_train_dataset, batch_size=32, shuffle=True)\n",
        "diabetes_val_loader = DataLoader(diabetes_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "diabetes_model = DiseaseMotifGNN(input_dim=input_dim, hidden_dim=64, num_layers=3)\n",
        "\n",
        "diabetes_trained_model, diabetes_history = train_model(\n",
        "    diabetes_model,\n",
        "    diabetes_train_loader,\n",
        "    diabetes_val_loader,\n",
        "    num_epochs=30,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "torch.save(diabetes_trained_model.state_dict(), 'diabetes_gnn_model.pt')\n",
        "print(\"Diabetes GNN model saved to 'diabetes_gnn_model.pt'\")\n",
        "\n",
        "# COPD model\n",
        "copd_train_indices, copd_val_indices = train_test_split(\n",
        "    range(len(copd_dataset)),\n",
        "    test_size=0.2,\n",
        "    stratify=copd_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "copd_train_dataset = [copd_dataset[i] for i in copd_train_indices]\n",
        "copd_val_dataset = [copd_dataset[i] for i in copd_val_indices]\n",
        "\n",
        "copd_train_loader = DataLoader(copd_train_dataset, batch_size=32, shuffle=True)\n",
        "copd_val_loader = DataLoader(copd_val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "copd_model = DiseaseMotifGNN(input_dim=input_dim, hidden_dim=64, num_layers=3)\n",
        "\n",
        "copd_trained_model, copd_history = train_model(\n",
        "    copd_model,\n",
        "    copd_train_loader,\n",
        "    copd_val_loader,\n",
        "    num_epochs=30,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "torch.save(copd_trained_model.state_dict(), 'copd_gnn_model.pt')\n",
        "print(\"COPD GNN model saved to 'copd_gnn_model.pt'\")\n",
        "'''\n",
        "\n",
        "# Save all dataset information\n",
        "with open('disease_datasets.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'afib': (afib_dataset, afib_labels),\n",
        "        'diabetes': (diabetes_dataset, diabetes_labels),\n",
        "        'copd': (copd_dataset, copd_labels)\n",
        "    }, f)\n",
        "\n",
        "print(\"All disease datasets saved to 'disease_datasets.pkl'\")"
      ],
      "metadata": {
        "id": "yKVsGM04AV0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "from torch_geometric.data import Data, Batch\n",
        "import os\n",
        "\n",
        "# Load the graph, data, and models\n",
        "with open('augmented_phenotype_graph.pkl', 'rb') as f:\n",
        "    G_aug = pickle.load(f)\n",
        "\n",
        "# Load trained model\n",
        "model = DiseaseMotifGNN(input_dim=206, hidden_dim=64, num_layers=3)  # 200 embedding dims + 6 features\n",
        "model.load_state_dict(torch.load('afib_gnn_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded augmented graph with {len(G_aug.nodes)} nodes and {len(G_aug.edges)} edges\")\n",
        "print(\"Loaded AFib detection model\")\n",
        "\n",
        "class SlidingWindowInference:\n",
        "    def __init__(self, graph, model, window_size=60, stride=1):\n",
        "        \"\"\"\n",
        "        Sliding window inference for real-time disease detection\n",
        "\n",
        "        Args:\n",
        "            graph: NetworkX graph\n",
        "            model: Trained GNN model\n",
        "            window_size: Window size in nodes\n",
        "            stride: Stride for sliding window\n",
        "        \"\"\"\n",
        "        self.G = graph\n",
        "        self.model = model\n",
        "        self.window_size = window_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # Sort nodes by timestamp\n",
        "        self.sorted_nodes = sorted(\n",
        "            list(self.G.nodes()),\n",
        "            key=lambda n: self.G.nodes[n]['timestamp']\n",
        "        )\n",
        "\n",
        "        # Define node features to extract\n",
        "        self.node_features = [\n",
        "            'hr', 'spo2', 'activity_level', 'sleep_stage',\n",
        "            'stress', 'hrv'\n",
        "        ]\n",
        "\n",
        "        # Risk score history\n",
        "        self.timestamps = []\n",
        "        self.risk_scores = []\n",
        "\n",
        "        # Device\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def get_window_data(self, start_idx):\n",
        "        \"\"\"\n",
        "        Extract a PyTorch Geometric Data object for a window\n",
        "\n",
        "        Args:\n",
        "            start_idx: Starting index in sorted_nodes\n",
        "\n",
        "        Returns:\n",
        "            PyTorch Geometric Data object\n",
        "        \"\"\"\n",
        "        # Ensure we don't go out of bounds\n",
        "        if start_idx + self.window_size > len(self.sorted_nodes):\n",
        "            return None\n",
        "\n",
        "        # Get nodes in this window\n",
        "        window_nodes = self.sorted_nodes[start_idx:start_idx+self.window_size]\n",
        "\n",
        "        # Extract subgraph\n",
        "        subgraph = self.G.subgraph(window_nodes)\n",
        "\n",
        "        # Create node mapping (for edge indices)\n",
        "        node_mapping = {node: i for i, node in enumerate(window_nodes)}\n",
        "\n",
        "        # Get node features\n",
        "        node_embeddings = []\n",
        "        node_features = []\n",
        "\n",
        "        for node in window_nodes:\n",
        "            # Get embedding\n",
        "            embedding = self.G.nodes[node]['embedding']\n",
        "            node_embeddings.append(embedding)\n",
        "\n",
        "            # Get other features\n",
        "            features = [self.G.nodes[node].get(feat, 0) for feat in self.node_features]\n",
        "            node_features.append(features)\n",
        "\n",
        "        # Combine embeddings and features\n",
        "        node_embeddings = torch.tensor(np.array(node_embeddings), dtype=torch.float)\n",
        "        node_features = torch.tensor(np.array(node_features), dtype=torch.float)\n",
        "\n",
        "        # Normalize node features\n",
        "        node_features = (node_features - node_features.mean(dim=0)) / (node_features.std(dim=0) + 1e-6)\n",
        "\n",
        "        # Combine all node features\n",
        "        x = torch.cat([node_embeddings, node_features], dim=1)\n",
        "\n",
        "        # Get edges\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "\n",
        "        for u, v, data in subgraph.edges(data=True):\n",
        "            # Convert node IDs to local indices\n",
        "            src_idx = node_mapping[u]\n",
        "            dst_idx = node_mapping[v]\n",
        "\n",
        "            edge_index.append([src_idx, dst_idx])\n",
        "\n",
        "            # Edge attributes\n",
        "            edge_type = 1.0 if data.get('edge_type') == 'sequential' else 0.0\n",
        "            is_virtual = 1.0 if data.get('is_virtual', False) else 0.0\n",
        "            weight = data.get('weight', 1.0)\n",
        "\n",
        "            edge_attr.append([edge_type, is_virtual, weight])\n",
        "\n",
        "        # Convert to PyTorch tensors\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "        # Create Data object\n",
        "        data = Data(\n",
        "            x=x,\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=edge_attr,\n",
        "            num_nodes=len(window_nodes)\n",
        "        )\n",
        "\n",
        "        return data\n",
        "\n",
        "    def run_inference(self, num_windows=None, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Run inference on sliding windows\n",
        "\n",
        "        Args:\n",
        "            num_windows: Number of windows to process (None = all)\n",
        "            threshold: Risk score threshold for alert\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with timestamps and risk scores\n",
        "        \"\"\"\n",
        "        # Reset history\n",
        "        self.timestamps = []\n",
        "        self.risk_scores = []\n",
        "        self.alerts = []\n",
        "\n",
        "        # Calculate max windows\n",
        "        max_windows = len(self.sorted_nodes) - self.window_size\n",
        "        if num_windows is None:\n",
        "            num_windows = max_windows\n",
        "        else:\n",
        "            num_windows = min(num_windows, max_windows)\n",
        "\n",
        "        # Run inference\n",
        "        for i in tqdm(range(0, num_windows, self.stride), desc=\"Running sliding window inference\"):\n",
        "            # Get window data\n",
        "            data = self.get_window_data(i)\n",
        "            if data is None:\n",
        "                continue\n",
        "\n",
        "            # Get middle node's timestamp\n",
        "            mid_idx = i + self.window_size // 2\n",
        "            timestamp = self.G.nodes[self.sorted_nodes[mid_idx]]['timestamp']\n",
        "\n",
        "            # Run inference\n",
        "            with torch.no_grad():\n",
        "                data = data.to(self.device)\n",
        "                batch = Batch.from_data_list([data])\n",
        "                risk_score = self.model(batch).item()\n",
        "\n",
        "            # Check for alert\n",
        "            is_alert = risk_score > threshold\n",
        "\n",
        "            # Store results\n",
        "            self.timestamps.append(timestamp)\n",
        "            self.risk_scores.append(risk_score)\n",
        "            self.alerts.append(is_alert)\n",
        "\n",
        "        # Create DataFrame\n",
        "        results = pd.DataFrame({\n",
        "            'timestamp': self.timestamps,\n",
        "            'risk_score': self.risk_scores,\n",
        "            'alert': self.alerts\n",
        "        })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_risk_scores(self, results=None, figsize=(12, 6), threshold=0.5):\n",
        "        \"\"\"\n",
        "        Plot risk scores over time\n",
        "\n",
        "        Args:\n",
        "            results: DataFrame with results (if None, use stored results)\n",
        "            figsize: Figure size\n",
        "            threshold: Risk score threshold to highlight\n",
        "        \"\"\"\n",
        "        if results is None:\n",
        "            results = pd.DataFrame({\n",
        "                'timestamp': self.timestamps,\n",
        "                'risk_score': self.risk_scores,\n",
        "                'alert': self.alerts\n",
        "            })\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "\n",
        "        # Plot risk scores\n",
        "        plt.plot(results['timestamp'], results['risk_score'], 'b-', label='Risk Score')\n",
        "\n",
        "        # Plot threshold\n",
        "        plt.axhline(y=threshold, color='r', linestyle='--', label='Alert Threshold')\n",
        "\n",
        "        # Highlight alerts\n",
        "        alerts = results[results['alert']]\n",
        "        if len(alerts) > 0:\n",
        "            plt.scatter(alerts['timestamp'], alerts['risk_score'],\n",
        "                       color='red', s=50, label='Alert')\n",
        "\n",
        "        plt.title('Disease Risk Score Over Time')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Risk Score')\n",
        "        plt.ylim(0, 1)\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.savefig('risk_scores.png')\n",
        "        plt.show()\n",
        "\n",
        "    def save_results(self, results, filename='risk_scores.csv'):\n",
        "        \"\"\"Save inference results to CSV\"\"\"\n",
        "        results.to_csv(filename, index=False)\n",
        "        print(f\"Results saved to '{filename}'\")\n",
        "\n",
        "# Create sliding window inference\n",
        "inference = SlidingWindowInference(G_aug, model, window_size=60, stride=5)\n",
        "\n",
        "# Run inference\n",
        "results = inference.run_inference(threshold=0.5)\n",
        "print(f\"Generated {len(results)} risk assessments\")\n",
        "\n",
        "# Plot risk scores\n",
        "inference.plot_risk_scores(results)\n",
        "\n",
        "# Save results\n",
        "inference.save_results(results)\n",
        "\n",
        "# Count alerts\n",
        "num_alerts = results['alert'].sum()\n",
        "print(f\"Detected {num_alerts} potential disease events\")\n",
        "\n",
        "# Process the highest risk windows for analysis\n",
        "high_risk_windows = results.nlargest(5, 'risk_score')\n",
        "print(\"Top 5 highest risk windows:\")\n",
        "print(high_risk_windows)"
      ],
      "metadata": {
        "id": "ck9Z3X1TAdML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from scipy.stats import beta\n",
        "import pickle\n",
        "\n",
        "# Load the risk scores\n",
        "risk_scores = pd.read_csv('risk_scores.csv')\n",
        "print(f\"Loaded {len(risk_scores)} risk assessments\")\n",
        "\n",
        "class BayesianPersonalization:\n",
        "    def __init__(self, initial_prevalence=0.05):\n",
        "        \"\"\"\n",
        "        Bayesian personalization for disease risk calibration\n",
        "\n",
        "        Args:\n",
        "            initial_prevalence: Initial disease prevalence (prior)\n",
        "        \"\"\"\n",
        "        self.prevalence = initial_prevalence\n",
        "\n",
        "        # Beta distribution parameters for prior\n",
        "        # Shape the beta to have mean = prevalence\n",
        "        self.alpha = 1 + initial_prevalence * 10  # Adding 1 for stability\n",
        "        self.beta = 1 + (1 - initial_prevalence) * 10  # Adding 1 for stability\n",
        "\n",
        "        # History of parameter updates\n",
        "        self.alpha_history = [self.alpha]\n",
        "        self.beta_history = [self.beta]\n",
        "        self.prevalence_history = [self.prevalence]\n",
        "\n",
        "        # User feedback history\n",
        "        self.feedback_history = []\n",
        "\n",
        "        print(f\"Initialized Bayesian personalization with prevalence {initial_prevalence:.4f}\")\n",
        "        print(f\"Initial beta parameters: alpha={self.alpha:.4f}, beta={self.beta:.4f}\")\n",
        "\n",
        "    def update_prior(self, risk_score, user_confirmed):\n",
        "        \"\"\"\n",
        "        Update prior based on user feedback\n",
        "\n",
        "        Args:\n",
        "            risk_score: Model's risk score (0-1)\n",
        "            user_confirmed: Whether user confirmed the alert (True/False)\n",
        "        \"\"\"\n",
        "        # Store feedback\n",
        "        self.feedback_history.append((risk_score, user_confirmed))\n",
        "\n",
        "        # Update parameters\n",
        "        if user_confirmed:\n",
        "            # True positive: Increase alpha (more confidence in positive)\n",
        "            # Weight by risk score (more confident update for high scores)\n",
        "            self.alpha += risk_score\n",
        "        else:\n",
        "            # False positive: Increase beta (more confidence in negative)\n",
        "            # Weight by risk score\n",
        "            self.beta += risk_score\n",
        "\n",
        "        # Update prevalence\n",
        "        self.prevalence = self.alpha / (self.alpha + self.beta)\n",
        "\n",
        "        # Update history\n",
        "        self.alpha_history.append(self.alpha)\n",
        "        self.beta_history.append(self.beta)\n",
        "        self.prevalence_history.append(self.prevalence)\n",
        "\n",
        "        print(f\"Updated prevalence to {self.prevalence:.4f}\")\n",
        "        print(f\"New beta parameters: alpha={self.alpha:.4f}, beta={self.beta:.4f}\")\n",
        "\n",
        "    def calibrate_risk(self, risk_score):\n",
        "        \"\"\"\n",
        "        Calibrate risk score based on current prior\n",
        "\n",
        "        Args:\n",
        "            risk_score: Model's raw risk score\n",
        "\n",
        "        Returns:\n",
        "            Calibrated risk score\n",
        "        \"\"\"\n",
        "        # Simple Bayesian calibration using prevalence\n",
        "        # This is a simplified approach; more sophisticated methods exist\n",
        "\n",
        "        # Adjust using Bayes' rule: P(A|B) = P(B|A)P(A)/P(B)\n",
        "        # where P(A) is the prevalence, P(B|A) is the risk score\n",
        "        # We'll treat the risk score as a likelihood ratio\n",
        "\n",
        "        # Avoid division by zero\n",
        "        risk_score = np.clip(risk_score, 0.01, 0.99)\n",
        "\n",
        "        # Calculate likelihood ratio\n",
        "        lr = risk_score / (1 - risk_score)\n",
        "\n",
        "        # Apply Bayes' rule\n",
        "        calibrated = (self.prevalence * lr) / (self.prevalence * lr + (1 - self.prevalence))\n",
        "\n",
        "        return calibrated\n",
        "\n",
        "    def plot_beta_distribution(self, figsize=(10, 6)):\n",
        "        \"\"\"Plot the current beta distribution\"\"\"\n",
        "        x = np.linspace(0, 1, 1000)\n",
        "        y = beta.pdf(x, self.alpha, self.beta)\n",
        "\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.plot(x, y, 'b-', lw=2, label='Current Beta Distribution')\n",
        "\n",
        "        # Plot initial distribution\n",
        "        y_initial = beta.pdf(x, self.alpha_history[0], self.beta_history[0])\n",
        "        plt.plot(x, y_initial, 'r--', lw=2, label='Initial Beta Distribution')\n",
        "\n",
        "        # Highlight the mean (prevalence)\n",
        "        plt.axvline(x=self.prevalence, color='g', linestyle='-',\n",
        "                   label=f'Current Mean: {self.prevalence:.4f}')\n",
        "        plt.axvline(x=self.prevalence_history[0], color='orange', linestyle='--',\n",
        "                   label=f'Initial Mean: {self.prevalence_history[0]:.4f}')\n",
        "\n",
        "        plt.title('Personalized Disease Prevalence Prior')\n",
        "        plt.xlabel('Prevalence')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.savefig('beta_distribution.png')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_prevalence_history(self, figsize=(10, 6)):\n",
        "        \"\"\"Plot the history of prevalence updates\"\"\"\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.plot(range(len(self.prevalence_history)), self.prevalence_history, 'b-', marker='o')\n",
        "\n",
        "        plt.title('Personalized Disease Prevalence History')\n",
        "        plt.xlabel('Update')\n",
        "        plt.ylabel('Prevalence')\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.savefig('prevalence_history.png')\n",
        "        plt.show()\n",
        "\n",
        "    def save_state(self, filename='bayesian_personalization.pkl'):\n",
        "        \"\"\"Save the current state\"\"\"\n",
        "        state = {\n",
        "            'alpha': self.alpha,\n",
        "            'beta': self.beta,\n",
        "            'prevalence': self.prevalence,\n",
        "            'alpha_history': self.alpha_history,\n",
        "            'beta_history': self.beta_history,\n",
        "            'prevalence_history': self.prevalence_history,\n",
        "            'feedback_history': self.feedback_history\n",
        "        }\n",
        "\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(state, f)\n",
        "\n",
        "        print(f\"Bayesian personalization state saved to '{filename}'\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_state(cls, filename='bayesian_personalization.pkl'):\n",
        "        \"\"\"Load state from file\"\"\"\n",
        "        with open(filename, 'rb') as f:\n",
        "            state = pickle.load(f)\n",
        "\n",
        "        # Create instance\n",
        "        instance = cls.__new__(cls)\n",
        "\n",
        "        # Set attributes\n",
        "        instance.alpha = state['alpha']\n",
        "        instance.beta = state['beta']\n",
        "        instance.prevalence = state['prevalence']\n",
        "        instance.alpha_history = state['alpha_history']\n",
        "        instance.beta_history = state['beta_history']\n",
        "        instance.prevalence_history = state['prevalence_history']\n",
        "        instance.feedback_history = state['feedback_history']\n",
        "\n",
        "        print(f\"Loaded Bayesian personalization state with prevalence {instance.prevalence:.4f}\")\n",
        "\n",
        "        return instance\n",
        "\n",
        "# Simulate user feedback for demonstration\n",
        "def simulate_user_feedback(risk_scores, true_positive_rate=0.7, false_positive_rate=0.2):\n",
        "    \"\"\"\n",
        "    Simulate user feedback on alerts\n",
        "\n",
        "    Args:\n",
        "        risk_scores: DataFrame with risk scores\n",
        "        true_positive_rate: Rate at which high-risk scores are confirmed\n",
        "        false_positive_rate: Rate at which low-risk scores are confirmed\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with added user_confirmed column\n",
        "    \"\"\"\n",
        "    df = risk_scores.copy()\n",
        "\n",
        "    # Simulate user confirmation based on risk score\n",
        "    user_confirmed = []\n",
        "\n",
        "    for score in df['risk_score']:\n",
        "        if score > 0.5:  # High risk\n",
        "            confirmed = np.random.random() < true_positive_rate\n",
        "        else:  # Low risk\n",
        "            confirmed = np.random.random() < false_positive_rate\n",
        "\n",
        "        user_confirmed.append(confirmed)\n",
        "\n",
        "    df['user_confirmed'] = user_confirmed\n",
        "\n",
        "    return df\n",
        "\n",
        "# Initial disease prevalence (we'll use approximate real-world values)\n",
        "# AFib prevalence in general population ~2%\n",
        "afib_prevalence = 0.02\n",
        "\n",
        "# Create Bayesian personalizer\n",
        "personalizer = BayesianPersonalization(initial_prevalence=afib_prevalence)\n",
        "\n",
        "# Plot initial distribution\n",
        "personalizer.plot_beta_distribution()\n",
        "\n",
        "# Simulate user feedback\n",
        "feedback_data = simulate_user_feedback(risk_scores)\n",
        "\n",
        "# Process some user feedback\n",
        "for i, (_, row) in enumerate(feedback_data.sample(10).iterrows()):\n",
        "    print(f\"\\nProcessing feedback {i+1}:\")\n",
        "    print(f\"  Risk score: {row['risk_score']:.4f}\")\n",
        "    print(f\"  User confirmed: {row['user_confirmed']}\")\n",
        "\n",
        "    # Update prior based on feedback\n",
        "    personalizer.update_prior(row['risk_score'], row['user_confirmed'])\n",
        "\n",
        "# Plot updated distribution\n",
        "personalizer.plot_beta_distribution()\n",
        "\n",
        "# Plot prevalence history\n",
        "personalizer.plot_prevalence_history()\n",
        "\n",
        "# Calibrate all risk scores\n",
        "calibrated_scores = []\n",
        "for score in risk_scores['risk_score']:\n",
        "    calibrated = personalizer.calibrate_risk(score)\n",
        "    calibrated_scores.append(calibrated)\n",
        "\n",
        "risk_scores['calibrated_risk'] = calibrated_scores\n",
        "\n",
        "# Compare original vs. calibrated scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(risk_scores['timestamp'], risk_scores['risk_score'], 'b-', label='Original Risk Score')\n",
        "plt.plot(risk_scores['timestamp'], risk_scores['calibrated_risk'], 'g-', label='Calibrated Risk Score')\n",
        "\n",
        "plt.title('Original vs. Calibrated Risk Scores')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Risk Score')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.savefig('original_vs_calibrated.png')\n",
        "plt.show()\n",
        "\n",
        "# Save personalizer state\n",
        "personalizer.save_state()\n",
        "\n",
        "# Save updated risk scores\n",
        "risk_scores.to_csv('calibrated_risk_scores.csv', index=False)\n",
        "print(\"Calibrated risk scores saved to 'calibrated_risk_scores.csv'\")"
      ],
      "metadata": {
        "id": "2Mg3Vg2xAePN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "\n",
        "# Load all necessary data\n",
        "risk_scores = pd.read_csv('calibrated_risk_scores.csv')\n",
        "with open('augmented_phenotype_graph.pkl', 'rb') as f:\n",
        "    G_aug = pickle.load(f)\n",
        "with open('bayesian_personalization.pkl', 'rb') as f:\n",
        "    bayes_state = pickle.load(f)\n",
        "\n",
        "print(f\"Loaded {len(risk_scores)} risk assessments\")\n",
        "print(f\"Loaded graph with {len(G_aug.nodes)} nodes and {len(G_aug.edges)} edges\")\n",
        "\n",
        "class HealthMonitor:\n",
        "    def __init__(self, risk_scores, graph, bayes_state=None):\n",
        "        \"\"\"\n",
        "        Visualization and monitoring dashboard for GET-Phen\n",
        "\n",
        "        Args:\n",
        "            risk_scores: DataFrame with risk scores\n",
        "            graph: NetworkX graph\n",
        "            bayes_state: Bayesian personalization state\n",
        "        \"\"\"\n",
        "        self.risk_scores = risk_scores\n",
        "        self.G = graph\n",
        "        self.bayes_state = bayes_state\n",
        "\n",
        "        # Extract physiological data from graph\n",
        "        self.extract_physiological_data()\n",
        "\n",
        "    def extract_physiological_data(self):\n",
        "        \"\"\"Extract physiological time series from graph nodes\"\"\"\n",
        "        # Sort nodes by timestamp\n",
        "        sorted_nodes = sorted(\n",
        "            list(self.G.nodes()),\n",
        "            key=lambda n: self.G.nodes[n]['timestamp']\n",
        "        )\n",
        "\n",
        "        # Extract data\n",
        "        timestamps = []\n",
        "        hr_values = []\n",
        "        spo2_values = []\n",
        "        activity_values = []\n",
        "        stress_values = []\n",
        "        hrv_values = []\n",
        "        sleep_values = []\n",
        "        is_virtual = []\n",
        "\n",
        "        for node in sorted_nodes:\n",
        "            data = self.G.nodes[node]\n",
        "\n",
        "            timestamps.append(data['timestamp'])\n",
        "            hr_values.append(data['hr'])\n",
        "            spo2_values.append(data['spo2'])\n",
        "            activity_values.append(data['activity_level'])\n",
        "            stress_values.append(data['stress'])\n",
        "            hrv_values.append(data['hrv'])\n",
        "            sleep_values.append(data['sleep_stage'])\n",
        "            is_virtual.append(data.get('is_virtual', False))\n",
        "\n",
        "        # Create DataFrame\n",
        "        self.physio_data = pd.DataFrame({\n",
        "            'timestamp': timestamps,\n",
        "            'hr': hr_values,\n",
        "            'spo2': spo2_values,\n",
        "            'activity': activity_values,\n",
        "            'stress': stress_values,\n",
        "            'hrv': hrv_values,\n",
        "            'sleep': sleep_values,\n",
        "            'is_virtual': is_virtual\n",
        "        })\n",
        "\n",
        "        print(f\"Extracted physiological data with {len(self.physio_data)} time points\")\n",
        "\n",
        "    def plot_risk_timeline(self, figsize=(15, 8)):\n",
        "        \"\"\"Plot risk scores and physiological data over time\"\"\"\n",
        "        # Create figure with subplots\n",
        "        fig, axes = plt.subplots(4, 1, figsize=figsize, sharex=True, gridspec_kw={'height_ratios': [2, 1, 1, 1]})\n",
        "\n",
        "        # Risk score timeline\n",
        "        ax1 = axes[0]\n",
        "        ax1.plot(self.risk_scores['timestamp'], self.risk_scores['risk_score'],\n",
        "                'b-', label='Original Risk')\n",
        "        ax1.plot(self.risk_scores['timestamp'], self.risk_scores['calibrated_risk'],\n",
        "                'g-', label='Calibrated Risk')\n",
        "\n",
        "        # Add alert threshold\n",
        "        ax1.axhline(y=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "\n",
        "        # Highlight alerts\n",
        "        alerts = self.risk_scores[self.risk_scores['alert']]\n",
        "        if len(alerts) > 0:\n",
        "            ax1.scatter(alerts['timestamp'], alerts['calibrated_risk'],\n",
        "                      color='red', s=50, label='Alert')\n",
        "\n",
        "        ax1.set_title('Disease Risk Timeline')\n",
        "        ax1.set_ylabel('Risk Score')\n",
        "        ax1.set_ylim(0, 1)\n",
        "        ax1.legend()\n",
        "        ax1.grid(alpha=0.3)\n",
        "\n",
        "        # HR and SpO2\n",
        "        ax2 = axes[1]\n",
        "        ax2.plot(self.physio_data['timestamp'], self.physio_data['hr'],\n",
        "                'r-', label='Heart Rate')\n",
        "        ax2.set_ylabel('Heart Rate (bpm)')\n",
        "        ax2.legend(loc='upper left')\n",
        "\n",
        "        ax2b = ax2.twinx()\n",
        "        ax2b.plot(self.physio_data['timestamp'], self.physio_data['spo2'],\n",
        "                 'b-', alpha=0.7, label='SpO2')\n",
        "        ax2b.set_ylabel('SpO2 (%)')\n",
        "        ax2b.set_ylim(90, 100)\n",
        "        ax2b.legend(loc='upper right')\n",
        "        ax2.grid(alpha=0.3)\n",
        "\n",
        "        # Activity and Stress\n",
        "        ax3 = axes[2]\n",
        "        ax3.plot(self.physio_data['timestamp'], self.physio_data['activity'],\n",
        "                'g-', label='Activity')\n",
        "        ax3.set_ylabel('Activity (0-1)')\n",
        "        ax3.set_ylim(0, 1)\n",
        "        ax3.legend(loc='upper left')\n",
        "\n",
        "        ax3b = ax3.twinx()\n",
        "        ax3b.plot(self.physio_data['timestamp'], self.physio_data['stress'],\n",
        "                 'orange', alpha=0.7, label='Stress')\n",
        "        ax3b.set_ylabel('Stress (0-100)')\n",
        "        ax3b.set_ylim(0, 100)\n",
        "        ax3b.legend(loc='upper right')\n",
        "        ax3.grid(alpha=0.3)\n",
        "\n",
        "        # Sleep and HRV\n",
        "        ax4 = axes[3]\n",
        "        ax4.plot(self.physio_data['timestamp'], self.physio_data['hrv'],\n",
        "                'purple', label='HRV')\n",
        "        ax4.set_ylabel('HRV (ms)')\n",
        "        ax4.legend(loc='upper left')\n",
        "        ax4.set_xlabel('Time')\n",
        "\n",
        "        ax4b = ax4.twinx()\n",
        "        # Plot sleep as a step function\n",
        "        ax4b.step(self.physio_data['timestamp'], self.physio_data['sleep'],\n",
        "                 'c-', where='post', alpha=0.7, label='Sleep Stage')\n",
        "        ax4b.set_ylabel('Sleep Stage')\n",
        "        ax4b.set_yticks([0, 1, 2, 3])\n",
        "        ax4b.set_yticklabels(['Wake', 'Light', 'Deep', 'REM'])\n",
        "        ax4b.legend(loc='upper right')\n",
        "        ax4.grid(alpha=0.3)\n",
        "\n",
        "        # Format x-axis\n",
        "        plt.gcf().autofmt_xdate()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('risk_timeline.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_risk_distribution(self, figsize=(12, 6)):\n",
        "        \"\"\"Plot distribution of risk scores\"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "        # Original risk distribution\n",
        "        sns.histplot(self.risk_scores['risk_score'], bins=20, kde=True, ax=axes[0])\n",
        "        axes[0].set_title('Original Risk Score Distribution')\n",
        "        axes[0].set_xlabel('Risk Score')\n",
        "        axes[0].set_ylabel('Frequency')\n",
        "        axes[0].axvline(x=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "        axes[0].legend()\n",
        "\n",
        "        # Calibrated risk distribution\n",
        "        sns.histplot(self.risk_scores['calibrated_risk'], bins=20, kde=True, ax=axes[1])\n",
        "        axes[1].set_title('Calibrated Risk Score Distribution')\n",
        "        axes[1].set_xlabel('Risk Score')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].axvline(x=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "        axes[1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('risk_distribution.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_snapshots(self, timestamps, figsize=(15, 12), window_size=60):\n",
        "        \"\"\"\n",
        "        Plot graph snapshots at specified timestamps\n",
        "\n",
        "        Args:\n",
        "            timestamps: List of timestamps to plot\n",
        "            figsize: Figure size\n",
        "            window_size: Number of nodes to include in each snapshot\n",
        "        \"\"\"\n",
        "        n_plots = len(timestamps)\n",
        "        fig, axes = plt.subplots(1, n_plots, figsize=figsize)\n",
        "\n",
        "        if n_plots == 1:\n",
        "            axes = [axes]  # Make sure axes is iterable\n",
        "\n",
        "        # Custom colormap for sleep stages\n",
        "        sleep_colors = ['lightyellow', 'lightblue', 'blue', 'purple']\n",
        "        sleep_cmap = LinearSegmentedColormap.from_list('sleep', sleep_colors, N=4)\n",
        "\n",
        "        # Sort nodes by timestamp\n",
        "        sorted_nodes = sorted(\n",
        "            list(self.G.nodes()),\n",
        "            key=lambda n: self.G.nodes[n]['timestamp']\n",
        "        )\n",
        "\n",
        "        for i, timestamp in enumerate(timestamps):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Find closest node to timestamp\n",
        "            closest_node = min(\n",
        "                sorted_nodes,\n",
        "                key=lambda n: abs((self.G.nodes[n]['timestamp'] - timestamp).total_seconds())\n",
        "            )\n",
        "            closest_idx = sorted_nodes.index(closest_node)\n",
        "\n",
        "            # Get window around closest node\n",
        "            start_idx = max(0, closest_idx - window_size // 2)\n",
        "            end_idx = min(len(sorted_nodes), start_idx + window_size)\n",
        "            window_nodes = sorted_nodes[start_idx:end_idx]\n",
        "\n",
        "            # Extract subgraph\n",
        "            subgraph = self.G.subgraph(window_nodes)\n",
        "\n",
        "            # Create positions\n",
        "            pos = {}\n",
        "            for node in subgraph.nodes():\n",
        "                # Use timestamp for x-axis\n",
        "                node_time = subgraph.nodes[node]['timestamp']\n",
        "                time_diff = (node_time - timestamp).total_seconds() / 3600  # Hours\n",
        "\n",
        "                # Use activity level for y-axis\n",
        "                activity = subgraph.nodes[node]['activity_level']\n",
        "\n",
        "                pos[node] = (time_diff, activity)\n",
        "\n",
        "            # Create edge colors based on type\n",
        "            edge_colors = []\n",
        "            widths = []\n",
        "            for u, v, data in subgraph.edges(data=True):\n",
        "                if data.get('is_virtual', False):\n",
        "                    if data['edge_type'] == 'sequential':\n",
        "                        edge_colors.append('purple')\n",
        "                        widths.append(1)\n",
        "                    else:  # similarity\n",
        "                        edge_colors.append('orange')\n",
        "                        widths.append(0.5)\n",
        "                else:\n",
        "                    if data['edge_type'] == 'sequential':\n",
        "                        edge_colors.append('blue')\n",
        "                        widths.append(1)\n",
        "                    else:  # similarity\n",
        "                        edge_colors.append('red')\n",
        "                        widths.append(0.5)\n",
        "\n",
        "            # Node colors based on real/virtual status and sleep stage\n",
        "            node_colors = []\n",
        "            node_sizes = []\n",
        "            for node in subgraph.nodes():\n",
        "                data = subgraph.nodes[node]\n",
        "\n",
        "                if data.get('is_virtual', False):\n",
        "                    if data.get('virtual_type') == 'exercise':\n",
        "                        node_colors.append('lime')  # Virtual exercise\n",
        "                    else:\n",
        "                        node_colors.append('magenta')  # Virtual hypoxia\n",
        "                    node_sizes.append(50)\n",
        "                else:\n",
        "                    sleep = data['sleep_stage']\n",
        "                    node_colors.append(sleep_colors[int(sleep)])\n",
        "                    node_sizes.append(30)\n",
        "\n",
        "            # Draw graph\n",
        "            nx.draw_networkx(\n",
        "                subgraph, pos=pos,\n",
        "                with_labels=False,\n",
        "                node_size=node_sizes,\n",
        "                node_color=node_colors,\n",
        "                edge_color=edge_colors,\n",
        "                width=widths,\n",
        "                alpha=0.7,\n",
        "                ax=ax\n",
        "            )\n",
        "\n",
        "            # Set title and labels\n",
        "            ax.set_title(f'Graph Snapshot at {timestamp.strftime(\"%Y-%m-%d %H:%M\")}')\n",
        "            ax.set_xlabel('Time Difference (hours)')\n",
        "            ax.set_ylabel('Activity Level')\n",
        "            ax.set_xlim(-12, 12)  # ±12 hours\n",
        "\n",
        "            # Add legend to first plot only\n",
        "            if i == 0:\n",
        "                handles = [\n",
        "                    plt.Line2D([0], [0], color='blue', label='Sequential Edge'),\n",
        "                    plt.Line2D([0], [0], color='red', label='Similarity Edge'),\n",
        "                    plt.Line2D([0], [0], marker='o', color='lightyellow', label='Awake', linestyle=''),\n",
        "                    plt.Line2D([0], [0], marker='o', color='lightblue', label='Light Sleep', linestyle=''),\n",
        "                    plt.Line2D([0], [0], marker='o', color='blue', label='Deep Sleep', linestyle=''),\n",
        "                    plt.Line2D([0], [0], marker='o', color='purple', label='REM Sleep', linestyle=''),\n",
        "                    plt.Line2D([0], [0], marker='o', color='lime', label='Exercise', linestyle=''),\n",
        "                    plt.Line2D([0], [0], marker='o', color='magenta', label='Hypoxia', linestyle='')\n",
        "                ]\n",
        "                ax.legend(handles=handles, loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('graph_snapshots.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def create_interactive_dashboard(self):\n",
        "        \"\"\"Create an interactive dashboard using Plotly\"\"\"\n",
        "        # Merge risk scores with physiological data by closest timestamp\n",
        "        risk_scores_resampled = self.risk_scores.copy()\n",
        "        physio_data_resampled = self.physio_data.copy()\n",
        "\n",
        "        # Convert timestamps to pandas datetime\n",
        "        risk_scores_resampled['timestamp'] = pd.to_datetime(risk_scores_resampled['timestamp'])\n",
        "        physio_data_resampled['timestamp'] = pd.to_datetime(physio_data_resampled['timestamp'])\n",
        "\n",
        "        # Set timestamp as index\n",
        "        risk_scores_resampled.set_index('timestamp', inplace=True)\n",
        "        physio_data_resampled.set_index('timestamp', inplace=True)\n",
        "\n",
        "        # Resample to common frequency (5 minute intervals)\n",
        "        risk_resampled = risk_scores_resampled.resample('5T').mean().interpolate()\n",
        "        physio_resampled = physio_data_resampled.resample('5T').mean().interpolate()\n",
        "\n",
        "        # Merge data\n",
        "        merged_data = pd.merge(\n",
        "            risk_resampled, physio_resampled,\n",
        "            left_index=True, right_index=True,\n",
        "            how='outer'\n",
        "        )\n",
        "        merged_data = merged_data.interpolate()\n",
        "\n",
        "        # Reset index to get timestamp as column\n",
        "        merged_data.reset_index(inplace=True)\n",
        "\n",
        "        # Create plotly figure\n",
        "        fig = make_subplots(\n",
        "            rows=4, cols=1,\n",
        "            shared_xaxes=True,\n",
        "            vertical_spacing=0.05,\n",
        "            subplot_titles=('Disease Risk', 'Heart Rate & SpO2', 'Activity & Stress', 'HRV & Sleep'),\n",
        "            row_heights=[0.35, 0.25, 0.2, 0.2]\n",
        "        )\n",
        "\n",
        "        # Risk scores\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['risk_score'],\n",
        "            mode='lines',\n",
        "            name='Original Risk',\n",
        "            line=dict(color='blue')\n",
        "        ), row=1, col=1)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['calibrated_risk'],\n",
        "            mode='lines',\n",
        "            name='Calibrated Risk',\n",
        "            line=dict(color='green')\n",
        "        ), row=1, col=1)\n",
        "\n",
        "        # Add threshold line\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=[0.5] * len(merged_data),\n",
        "            mode='lines',\n",
        "            name='Alert Threshold',\n",
        "            line=dict(color='red', dash='dash')\n",
        "        ), row=1, col=1)\n",
        "\n",
        "        # Heart Rate and SpO2\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['hr'],\n",
        "            mode='lines',\n",
        "            name='Heart Rate',\n",
        "            line=dict(color='red')\n",
        "        ), row=2, col=1)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['spo2'],\n",
        "            mode='lines',\n",
        "            name='SpO2',\n",
        "            line=dict(color='cyan'),\n",
        "            yaxis='y2'\n",
        "        ), row=2, col=1)\n",
        "\n",
        "        # Activity and Stress\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['activity'],\n",
        "            mode='lines',\n",
        "            name='Activity',\n",
        "            line=dict(color='green')\n",
        "        ), row=3, col=1)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['stress'],\n",
        "            mode='lines',\n",
        "            name='Stress',\n",
        "            line=dict(color='orange'),\n",
        "            yaxis='y3'\n",
        "        ), row=3, col=1)\n",
        "\n",
        "        # HRV and Sleep\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['hrv'],\n",
        "            mode='lines',\n",
        "            name='HRV',\n",
        "            line=dict(color='purple')\n",
        "        ), row=4, col=1)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=merged_data['timestamp'],\n",
        "            y=merged_data['sleep'],\n",
        "            mode='lines',\n",
        "            name='Sleep Stage',\n",
        "            line=dict(color='darkblue', shape='hv'),\n",
        "            yaxis='y4'\n",
        "        ), row=4, col=1)\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            height=900,\n",
        "            width=1000,\n",
        "            title='GET-Phen Health Monitoring Dashboard',\n",
        "            xaxis=dict(title='Time'),\n",
        "            yaxis=dict(title='Risk Score', range=[0, 1]),\n",
        "            yaxis2=dict(title='Heart Rate (bpm) / SpO2 (%)', range=[60, 100]),\n",
        "            yaxis3=dict(title='Activity (0-1) / Stress (0-100)', range=[0, 100]),\n",
        "            yaxis4=dict(title='HRV (ms) / Sleep Stage', range=[0, 100]),\n",
        "            legend=dict(x=1.05, y=1, orientation='v'),\n",
        "            hovermode='x unified'\n",
        "        )\n",
        "\n",
        "        # Save as HTML\n",
        "        fig.write_html('interactive_dashboard.html')\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def plot_disease_motifs(self, num_motifs=3, figsize=(15, 15)):\n",
        "        \"\"\"\n",
        "        Plot the most significant disease motifs detected\n",
        "\n",
        "        Args:\n",
        "            num_motifs: Number of motifs to plot\n",
        "            figsize: Figure size\n",
        "        \"\"\"\n",
        "        # Find time windows with highest risk scores\n",
        "        high_risk_windows = self.risk_scores.nlargest(num_motifs, 'calibrated_risk')\n",
        "\n",
        "        # Convert timestamps to datetime if needed\n",
        "        high_risk_windows['timestamp'] = pd.to_datetime(high_risk_windows['timestamp'])\n",
        "\n",
        "        # Extract timestamps\n",
        "        timestamps = high_risk_windows['timestamp'].tolist()\n",
        "\n",
        "        # Plot graph snapshots\n",
        "        self.plot_snapshots(timestamps, figsize=figsize)\n",
        "\n",
        "    def create_correlation_matrix(self, figsize=(10, 8)):\n",
        "        \"\"\"Create a correlation matrix of physiological metrics and risk scores\"\"\"\n",
        "        # Merge risk scores with physiological data\n",
        "        risk_scores_df = self.risk_scores.copy()\n",
        "        physio_df = self.physio_data.copy()\n",
        "\n",
        "        # Convert timestamps to datetime\n",
        "        risk_scores_df['timestamp'] = pd.to_datetime(risk_scores_df['timestamp'])\n",
        "        physio_df['timestamp'] = pd.to_datetime(physio_df['timestamp'])\n",
        "\n",
        "        # Resample to common frequency\n",
        "        risk_scores_df.set_index('timestamp', inplace=True)\n",
        "        physio_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "        risk_resampled = risk_scores_df.resample('5T').mean().interpolate()\n",
        "        physio_resampled = physio_df.resample('5T').mean().interpolate()\n",
        "\n",
        "        # Merge data\n",
        "        merged_data = pd.merge(\n",
        "            risk_resampled, physio_resampled,\n",
        "            left_index=True, right_index=True,\n",
        "            how='outer'\n",
        "        )\n",
        "        merged_data = merged_data.interpolate()\n",
        "\n",
        "        # Drop non-numeric columns\n",
        "        numeric_data = merged_data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = numeric_data.corr()\n",
        "\n",
        "        # Create heatmap\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "        plt.title('Correlation Matrix of Health Metrics and Risk Scores')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('correlation_matrix.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        return corr_matrix\n",
        "\n",
        "    def save_monitoring_summary(self, filename='monitoring_summary.html'):\n",
        "        \"\"\"Save a comprehensive monitoring summary as HTML\"\"\"\n",
        "        import base64\n",
        "        from io import BytesIO\n",
        "\n",
        "        # Create HTML output\n",
        "        html_output = \"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>GET-Phen Health Monitoring Summary</title>\n",
        "            <style>\n",
        "                body {\n",
        "                    font-family: Arial, sans-serif;\n",
        "                    margin: 20px;\n",
        "                    line-height: 1.6;\n",
        "                }\n",
        "                .header {\n",
        "                    background-color: #4e73df;\n",
        "                    color: white;\n",
        "                    padding: 20px;\n",
        "                    text-align: center;\n",
        "                }\n",
        "                .section {\n",
        "                    margin: 20px 0;\n",
        "                    padding: 15px;\n",
        "                    border: 1px solid #ddd;\n",
        "                    border-radius: 5px;\n",
        "                }\n",
        "                .figure {\n",
        "                    margin: 20px 0;\n",
        "                    text-align: center;\n",
        "                }\n",
        "                .stats {\n",
        "                    display: flex;\n",
        "                    justify-content: space-around;\n",
        "                    flex-wrap: wrap;\n",
        "                }\n",
        "                .stat-box {\n",
        "                    background-color: #f8f9fc;\n",
        "                    border-left: 4px solid #4e73df;\n",
        "                    padding: 15px;\n",
        "                    margin: 10px;\n",
        "                    width: 200px;\n",
        "                }\n",
        "                table {\n",
        "                    border-collapse: collapse;\n",
        "                    width: 100%;\n",
        "                    margin: 20px 0;\n",
        "                }\n",
        "                th, td {\n",
        "                    border: 1px solid #ddd;\n",
        "                    padding: 8px;\n",
        "                    text-align: left;\n",
        "                }\n",
        "                th {\n",
        "                    background-color: #f2f2f2;\n",
        "                }\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"header\">\n",
        "                <h1>GET-Phen Health Monitoring Summary</h1>\n",
        "                <p>Generated on {date}</p>\n",
        "            </div>\n",
        "        \"\"\".format(date=datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
        "\n",
        "        # Add risk statistics section\n",
        "        html_output += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Risk Assessment Summary</h2>\n",
        "                <div class=\"stats\">\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Mean Risk Score</h3>\n",
        "                        <p>{mean_risk:.3f}</p>\n",
        "                    </div>\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Max Risk Score</h3>\n",
        "                        <p>{max_risk:.3f}</p>\n",
        "                    </div>\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Total Alerts</h3>\n",
        "                        <p>{total_alerts}</p>\n",
        "                    </div>\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Alert Rate</h3>\n",
        "                        <p>{alert_rate:.1f}%</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        \"\"\".format(\n",
        "            mean_risk=self.risk_scores['calibrated_risk'].mean(),\n",
        "            max_risk=self.risk_scores['calibrated_risk'].max(),\n",
        "            total_alerts=self.risk_scores['alert'].sum(),\n",
        "            alert_rate=self.risk_scores['alert'].mean() * 100\n",
        "        )\n",
        "\n",
        "        # Add risk timeline figure\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.risk_scores['timestamp'], self.risk_scores['calibrated_risk'],\n",
        "                'g-', label='Calibrated Risk')\n",
        "        plt.axhline(y=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "        plt.title('Disease Risk Timeline')\n",
        "        plt.xlabel('Time')\n",
        "        plt.ylabel('Risk Score')\n",
        "        plt.legend()\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        # Save figure to BytesIO\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=100)\n",
        "        buf.seek(0)\n",
        "        img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        # Add figure to HTML\n",
        "        html_output += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Risk Timeline</h2>\n",
        "                <div class=\"figure\">\n",
        "                    <img src=\"data:image/png;base64,{img_data}\" alt=\"Risk Timeline\">\n",
        "                </div>\n",
        "            </div>\n",
        "        \"\"\".format(img_data=img_str)\n",
        "\n",
        "        # Add alerts table\n",
        "        alerts = self.risk_scores[self.risk_scores['alert']].copy()\n",
        "        alerts['timestamp'] = pd.to_datetime(alerts['timestamp']).dt.strftime(\"%Y-%m-%d %H:%M\")\n",
        "        alerts = alerts.sort_values('calibrated_risk', ascending=False).head(10)\n",
        "\n",
        "        if len(alerts) > 0:\n",
        "            alert_table = \"\"\"\n",
        "                <table>\n",
        "                    <tr>\n",
        "                        <th>Timestamp</th>\n",
        "                        <th>Risk Score</th>\n",
        "                        <th>Calibrated Risk</th>\n",
        "                    </tr>\n",
        "            \"\"\"\n",
        "\n",
        "            for _, row in alerts.iterrows():\n",
        "                alert_table += \"\"\"\n",
        "                    <tr>\n",
        "                        <td>{timestamp}</td>\n",
        "                        <td>{risk:.3f}</td>\n",
        "                        <td>{cal_risk:.3f}</td>\n",
        "                    </tr>\n",
        "                \"\"\".format(\n",
        "                    timestamp=row['timestamp'],\n",
        "                    risk=row['risk_score'],\n",
        "                    cal_risk=row['calibrated_risk']\n",
        "                )\n",
        "\n",
        "            alert_table += \"</table>\"\n",
        "\n",
        "            html_output += \"\"\"\n",
        "                <div class=\"section\">\n",
        "                    <h2>Top 10 Highest Risk Alerts</h2>\n",
        "                    {table}\n",
        "                </div>\n",
        "            \"\"\".format(table=alert_table)\n",
        "\n",
        "        # Add correlation matrix\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Create a small correlation matrix excluding virtual node data\n",
        "        merged_data = pd.merge(\n",
        "            self.risk_scores[['timestamp', 'calibrated_risk']],\n",
        "            self.physio_data[self.physio_data['is_virtual'] == False][\n",
        "                ['timestamp', 'hr', 'spo2', 'activity', 'stress', 'hrv']\n",
        "            ],\n",
        "            on='timestamp',\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        corr_matrix = merged_data.select_dtypes(include=['float64', 'int64']).corr()\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
        "        plt.title('Correlation of Health Metrics with Risk Scores')\n",
        "\n",
        "        # Save figure to BytesIO\n",
        "        buf = BytesIO()\n",
        "        plt.savefig(buf, format='png', dpi=100)\n",
        "        buf.seek(0)\n",
        "        img_str = base64.b64encode(buf.read()).decode('utf-8')\n",
        "        plt.close()\n",
        "\n",
        "        # Add correlation matrix to HTML\n",
        "        html_output += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Correlation Analysis</h2>\n",
        "                <div class=\"figure\">\n",
        "                    <img src=\"data:image/png;base64,{img_data}\" alt=\"Correlation Matrix\">\n",
        "                </div>\n",
        "                <p>This matrix shows how different health metrics correlate with disease risk scores.</p>\n",
        "            </div>\n",
        "        \"\"\".format(img_data=img_str)\n",
        "\n",
        "        # Add physiological stats\n",
        "        html_output += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Physiological Statistics</h2>\n",
        "                <div class=\"stats\">\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Mean Heart Rate</h3>\n",
        "                        <p>{mean_hr:.1f} bpm</p>\n",
        "                    </div>\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Mean SpO2</h3>\n",
        "                        <p>{mean_spo2:.1f}%</p>\n",
        "                    </div>\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Mean HRV</h3>\n",
        "                        <p>{mean_hrv:.1f} ms</p>\n",
        "                    </div>\n",
        "                    <div class=\"stat-box\">\n",
        "                        <h3>Mean Activity</h3>\n",
        "                        <p>{mean_activity:.2f}</p>\n",
        "                    </div>\n",
        "                </div>\n",
        "            </div>\n",
        "        \"\"\".format(\n",
        "            mean_hr=self.physio_data[self.physio_data['is_virtual'] == False]['hr'].mean(),\n",
        "            mean_spo2=self.physio_data[self.physio_data['is_virtual'] == False]['spo2'].mean(),\n",
        "            mean_hrv=self.physio_data[self.physio_data['is_virtual'] == False]['hrv'].mean(),\n",
        "            mean_activity=self.physio_data[self.physio_data['is_virtual'] == False]['activity'].mean()\n",
        "        )\n",
        "\n",
        "        # Close HTML\n",
        "        html_output += \"\"\"\n",
        "            <div class=\"section\">\n",
        "                <h2>Analysis Summary</h2>\n",
        "                <p>This report shows the results of GET-Phen disease detection from smartwatch data using graph neural networks.</p>\n",
        "                <p>The system has detected {alert_count} potential disease events that exceed the risk threshold.</p>\n",
        "                <p>The mean calibrated risk score is {mean_risk:.3f}.</p>\n",
        "                <p>The monitoring period spans from {start_date} to {end_date}.</p>\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>Next Steps</h2>\n",
        "                <ul>\n",
        "                    <li>Continue data collection for longer-term trends</li>\n",
        "                    <li>Refine the risk threshold based on validation data</li>\n",
        "                    <li>Add more disease-specific models</li>\n",
        "                    <li>Integrate with healthcare systems for alerts</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\".format(\n",
        "            alert_count=self.risk_scores['alert'].sum(),\n",
        "            mean_risk=self.risk_scores['calibrated_risk'].mean(),\n",
        "            start_date=pd.to_datetime(self.risk_scores['timestamp']).min().strftime(\"%Y-%m-%d %H:%M\"),\n",
        "            end_date=pd.to_datetime(self.risk_scores['timestamp']).max().strftime(\"%Y-%m-%d %H:%M\")\n",
        "        )\n",
        "\n",
        "        # Save HTML to file\n",
        "        with open(filename, 'w') as f:\n",
        "            f.write(html_output)\n",
        "\n",
        "        print(f\"Monitoring summary saved to '{filename}'\")\n",
        "\n",
        "\n",
        "# Create health monitor\n",
        "monitor = HealthMonitor(risk_scores, G_aug, bayes_state)\n",
        "\n",
        "# Create visualizations\n",
        "monitor.plot_risk_timeline()\n",
        "monitor.plot_risk_distribution()\n",
        "\n",
        "# Plot disease motifs\n",
        "monitor.plot_disease_motifs(num_motifs=3)\n",
        "\n",
        "# Create correlation matrix\n",
        "corr_matrix = monitor.create_correlation_matrix()\n",
        "\n",
        "# Create and display interactive dashboard\n",
        "dashboard = monitor.create_interactive_dashboard()\n",
        "# This will save the dashboard as HTML\n",
        "# To view it in Colab, you can use:\n",
        "from IPython.display import IFrame\n",
        "# Display the saved HTML (adjust width/height as needed)\n",
        "display(IFrame('interactive_dashboard.html', width=1000, height=800))\n",
        "\n",
        "# Save monitoring summary\n",
        "monitor.save_monitoring_summary()\n",
        "print(\"All visualizations and monitoring outputs have been generated.\")"
      ],
      "metadata": {
        "id": "LZfhXLCSGzkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if required files exist\n",
        "required_files = [\n",
        "    'smartwatch_data.pkl',\n",
        "    'user_meta.json',\n",
        "    'smartwatch_transformer.pt',\n",
        "    'smartwatch_embeddings.pkl',\n",
        "    'augmented_phenotype_graph.pkl',\n",
        "    'afib_gnn_model.pt',\n",
        "    'calibrated_risk_scores.csv',\n",
        "    'bayesian_personalization.pkl'\n",
        "]\n",
        "\n",
        "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"WARNING: The following required files are missing:\")\n",
        "    for file in missing_files:\n",
        "        print(f\"  - {file}\")\n",
        "    print(\"\\nPlease run the previous modules to generate these files.\")\n",
        "else:\n",
        "    print(\"All required files are present. Ready to run the GET-Phen framework.\")\n",
        "\n",
        "class GETPhen:\n",
        "    \"\"\"\n",
        "    Graph-Enabled Temporal Phenotyping (GET-Phen) Framework\n",
        "    Integration of all components for health monitoring from smartwatch data\n",
        "    \"\"\"\n",
        "    def __init__(self, load_pretrained=True):\n",
        "        \"\"\"\n",
        "        Initialize the GET-Phen framework\n",
        "\n",
        "        Args:\n",
        "            load_pretrained: Whether to load pretrained models and data\n",
        "        \"\"\"\n",
        "        self.models = {}\n",
        "        self.data = {}\n",
        "        self.components = {}\n",
        "\n",
        "        if load_pretrained:\n",
        "            self.load_pretrained_components()\n",
        "\n",
        "    def load_pretrained_components(self):\n",
        "        \"\"\"Load all pretrained models and data\"\"\"\n",
        "        print(\"Loading pretrained GET-Phen components...\")\n",
        "\n",
        "        try:\n",
        "            # Load smartwatch data\n",
        "            self.data['smartwatch_data'] = pd.read_pickle('smartwatch_data.pkl')\n",
        "\n",
        "            # Load user metadata\n",
        "            import json\n",
        "            with open('user_meta.json', 'r') as f:\n",
        "                self.data['user_meta'] = json.load(f)\n",
        "\n",
        "            # Load embeddings\n",
        "            self.data['embeddings'] = pd.read_pickle('smartwatch_embeddings.pkl')\n",
        "\n",
        "            # Load graph\n",
        "            with open('augmented_phenotype_graph.pkl', 'rb') as f:\n",
        "                self.data['graph'] = pickle.load(f)\n",
        "\n",
        "            # Load disease models\n",
        "            from disease_motif_gnn import DiseaseMotifGNN\n",
        "\n",
        "            # AFib model\n",
        "            afib_model = DiseaseMotifGNN(input_dim=206, hidden_dim=64, num_layers=3)\n",
        "            afib_model.load_state_dict(torch.load('afib_gnn_model.pt'))\n",
        "            afib_model.eval()\n",
        "            self.models['afib'] = afib_model\n",
        "\n",
        "            # Load risk scores\n",
        "            self.data['risk_scores'] = pd.read_csv('calibrated_risk_scores.csv')\n",
        "\n",
        "            # Load Bayesian personalization\n",
        "            from bayesian_updating import BayesianPersonalization\n",
        "            self.components['bayes'] = BayesianPersonalization.load_state('bayesian_personalization.pkl')\n",
        "\n",
        "            print(\"All components loaded successfully!\")\n",
        "\n",
        "            # Print data summary\n",
        "            self._print_data_summary()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading components: {e}\")\n",
        "            print(\"Please make sure all required modules have been run.\")\n",
        "\n",
        "    def _print_data_summary(self):\n",
        "        \"\"\"Print summary of loaded data\"\"\"\n",
        "        print(\"\\n=== GET-Phen Framework Data Summary ===\")\n",
        "        print(f\"Smartwatch data: {len(self.data['smartwatch_data'])} time points\")\n",
        "        print(f\"User metadata: {self.data['user_meta']}\")\n",
        "        print(f\"Embeddings: {len(self.data['embeddings'])} vectors\")\n",
        "        print(f\"Graph: {len(self.data['graph'].nodes)} nodes, {len(self.data['graph'].edges)} edges\")\n",
        "        print(f\"Risk scores: {len(self.data['risk_scores'])} predictions\")\n",
        "        print(f\"Models: {list(self.models.keys())}\")\n",
        "        print(f\"Components: {list(self.components.keys())}\")\n",
        "\n",
        "    def run_end_to_end_demo(self, demo_length_hours=24, real_time=False):\n",
        "        \"\"\"\n",
        "        Run an end-to-end demonstration of the GET-Phen framework\n",
        "\n",
        "        Args:\n",
        "            demo_length_hours: Number of hours of data to process\n",
        "            real_time: Whether to simulate real-time processing\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Running GET-Phen End-to-End Demo ===\")\n",
        "\n",
        "        # Prepare demo data - use a slice of the existing data\n",
        "        data = self.data['smartwatch_data'].copy()\n",
        "        data = data.iloc[:min(len(data), demo_length_hours * 60)]  # 1 minute intervals\n",
        "\n",
        "        # Initialize storage for demo results\n",
        "        timestamps = []\n",
        "        risk_scores = []\n",
        "        calibrated_scores = []\n",
        "        hrs = []\n",
        "        spo2s = []\n",
        "        activities = []\n",
        "        alerts = []\n",
        "\n",
        "        # Create fig for live plotting\n",
        "        if real_time:\n",
        "            plt.figure(figsize=(14, 8))\n",
        "            plt.ion()  # Enable interactive mode\n",
        "\n",
        "        # Import necessary modules for real-time processing\n",
        "        from self_supervised_model import SmartWatchTransformer\n",
        "        from online_inference import SlidingWindowInference\n",
        "\n",
        "        # Load models\n",
        "        model = SmartWatchTransformer(input_dim=11, hidden_dim=128, output_dim=200)\n",
        "        model.load_state_dict(torch.load('smartwatch_transformer.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        # Process data points\n",
        "        print(f\"Processing {len(data)} data points ({demo_length_hours} hours)...\")\n",
        "\n",
        "        window_size = 30  # Process in 30-minute windows\n",
        "        bayes = self.components['bayes']\n",
        "\n",
        "        for i in tqdm(range(0, len(data) - window_size, window_size)):\n",
        "            # Get current window\n",
        "            current_window = data.iloc[i:i+window_size]\n",
        "\n",
        "            # Extract features\n",
        "            timestamp = current_window.iloc[-1]['timestamp']\n",
        "            hr = current_window.iloc[-1]['hr']\n",
        "            spo2 = current_window.iloc[-1]['spo2']\n",
        "            activity = current_window.iloc[-1]['activity_level']\n",
        "\n",
        "            # Simulate generating embedding\n",
        "            embedding_idx = min(i + window_size // 2, len(self.data['embeddings']) - 1)\n",
        "            embedding = self.data['embeddings'].iloc[embedding_idx]['embedding']\n",
        "\n",
        "            # Simulate GNN risk prediction\n",
        "            risk_idx = min(i + window_size // 2, len(self.data['risk_scores']) - 1)\n",
        "            risk_score = self.data['risk_scores'].iloc[risk_idx]['risk_score']\n",
        "\n",
        "            # Calibrate score with Bayesian personalization\n",
        "            calibrated_score = bayes.calibrate_risk(risk_score)\n",
        "\n",
        "            # Check for alert\n",
        "            is_alert = calibrated_score > 0.5\n",
        "\n",
        "            # Store results\n",
        "            timestamps.append(timestamp)\n",
        "            risk_scores.append(risk_score)\n",
        "            calibrated_scores.append(calibrated_score)\n",
        "            hrs.append(hr)\n",
        "            spo2s.append(spo2)\n",
        "            activities.append(activity)\n",
        "            alerts.append(is_alert)\n",
        "\n",
        "            # Live plotting if real-time mode is enabled\n",
        "            if real_time and (i % (window_size * 2) == 0 or i == len(data) - window_size - 1):\n",
        "                clear_output(wait=True)\n",
        "\n",
        "                # Create plot\n",
        "                plt.clf()\n",
        "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
        "\n",
        "                # Plot risk scores\n",
        "                ax1.plot(timestamps, risk_scores, 'b-', label='Original Risk')\n",
        "                ax1.plot(timestamps, calibrated_scores, 'g-', label='Calibrated Risk')\n",
        "                ax1.axhline(y=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "\n",
        "                # Highlight alerts\n",
        "                alert_indices = [i for i, alert in enumerate(alerts) if alert]\n",
        "                if alert_indices:\n",
        "                    alert_times = [timestamps[i] for i in alert_indices]\n",
        "                    alert_scores = [calibrated_scores[i] for i in alert_indices]\n",
        "                    ax1.scatter(alert_times, alert_scores, color='red', s=80, marker='o', label='Alert')\n",
        "\n",
        "                ax1.set_title('GET-Phen Real-Time Disease Risk Monitoring')\n",
        "                ax1.set_ylabel('Risk Score')\n",
        "                ax1.set_ylim(0, 1)\n",
        "                ax1.legend()\n",
        "                ax1.grid(True, alpha=0.3)\n",
        "\n",
        "                # Plot physiological data\n",
        "                ax2.plot(timestamps, hrs, 'r-', label='Heart Rate')\n",
        "                ax2.set_ylabel('Heart Rate (bpm)')\n",
        "                ax2.legend(loc='upper left')\n",
        "\n",
        "                ax2b = ax2.twinx()\n",
        "                ax2b.plot(timestamps, spo2s, 'b-', alpha=0.7, label='SpO2')\n",
        "                ax2b.set_ylabel('SpO2 (%)')\n",
        "                ax2b.set_ylim(90, 100)\n",
        "                ax2b.legend(loc='upper right')\n",
        "\n",
        "                plt.xlabel('Time')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Pause to simulate real-time processing\n",
        "                time.sleep(1)\n",
        "\n",
        "        # Create final results dataframe\n",
        "        results = pd.DataFrame({\n",
        "            'timestamp': timestamps,\n",
        "            'risk_score': risk_scores,\n",
        "            'calibrated_risk': calibrated_scores,\n",
        "            'hr': hrs,\n",
        "            'spo2': spo2s,\n",
        "            'activity': activities,\n",
        "            'alert': alerts\n",
        "        })\n",
        "\n",
        "        # Final visualization\n",
        "        plt.ioff()  # Disable interactive mode\n",
        "\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
        "\n",
        "        # Plot risk scores\n",
        "        ax1.plot(results['timestamp'], results['risk_score'], 'b-', label='Original Risk')\n",
        "        ax1.plot(results['timestamp'], results['calibrated_risk'], 'g-', label='Calibrated Risk')\n",
        "        ax1.axhline(y=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "\n",
        "        # Highlight alerts\n",
        "        alerts = results[results['alert']]\n",
        "        if len(alerts) > 0:\n",
        "            ax1.scatter(alerts['timestamp'], alerts['calibrated_risk'], color='red', s=80, marker='o', label='Alert')\n",
        "\n",
        "        ax1.set_title('GET-Phen End-to-End Demo Results')\n",
        "        ax1.set_ylabel('Risk Score')\n",
        "        ax1.set_ylim(0, 1)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot physiological data\n",
        "        ax2.plot(results['timestamp'], results['hr'], 'r-', label='Heart Rate')\n",
        "        ax2.set_ylabel('Heart Rate (bpm)')\n",
        "        ax2.legend(loc='upper left')\n",
        "\n",
        "        ax2b = ax2.twinx()\n",
        "        ax2b.plot(results['timestamp'], results['spo2'], 'b-', alpha=0.7, label='SpO2')\n",
        "        ax2b.set_ylabel('SpO2 (%)')\n",
        "        ax2b.set_ylim(90, 100)\n",
        "        ax2b.legend(loc='upper right')\n",
        "\n",
        "        plt.xlabel('Time')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('getphen_demo_results.png', dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n=== GET-Phen Demo Results ===\")\n",
        "        print(f\"Processed {len(results)} time points over {demo_length_hours} hours\")\n",
        "        print(f\"Detected {sum(results['alert'])} potential disease events\")\n",
        "        print(f\"Mean risk score: {results['risk_score'].mean():.4f}\")\n",
        "        print(f\"Mean calibrated score: {results['calibrated_risk'].mean():.4f}\")\n",
        "\n",
        "        # Return results\n",
        "        return results\n",
        "\n",
        "    def process_new_data(self, new_data, window_size=60):\n",
        "        \"\"\"\n",
        "        Process new smartwatch data through the full GET-Phen pipeline\n",
        "\n",
        "        Args:\n",
        "            new_data: DataFrame with new smartwatch data\n",
        "            window_size: Size of processing window\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with risk scores and alerts\n",
        "        \"\"\"\n",
        "        print(f\"Processing {len(new_data)} new data points...\")\n",
        "\n",
        "        # Setup components\n",
        "        from self_supervised_model import SmartWatchTransformer, SmartWatchDataset\n",
        "        from disease_motif_gnn import DiseaseMotifGNN\n",
        "\n",
        "        try:\n",
        "            # Step 1: Generate embeddings\n",
        "            model = SmartWatchTransformer(input_dim=11, hidden_dim=128, output_dim=200)\n",
        "            model.load_state_dict(torch.load('smartwatch_transformer.pt'))\n",
        "            model.eval()\n",
        "\n",
        "            # Create dataset from new data\n",
        "            dataset = SmartWatchDataset(new_data, window_size=window_size, stride=window_size//2)\n",
        "\n",
        "            # Generate embeddings\n",
        "            embeddings = []\n",
        "            indices = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i in range(len(dataset)):\n",
        "                    batch = dataset[i]\n",
        "                    masked_data = batch['masked_data'].unsqueeze(1)  # Add batch dimension\n",
        "\n",
        "                    # Forward pass to get embeddings\n",
        "                    batch_embeddings, _ = model(masked_data)\n",
        "\n",
        "                    # Take mean embedding for the window\n",
        "                    mean_embedding = batch_embeddings.mean(dim=0).numpy()\n",
        "\n",
        "                    # Store embeddings and indices\n",
        "                    embeddings.append(mean_embedding)\n",
        "                    indices.append((batch['start_idx'] + batch['end_idx']) // 2)\n",
        "\n",
        "            # Step 2: Create temporary graph nodes and edges\n",
        "            G_temp = nx.DiGraph()\n",
        "\n",
        "            # Add nodes with embeddings\n",
        "            for i, (idx, embedding) in enumerate(zip(indices, embeddings)):\n",
        "                node_data = new_data.iloc[idx].copy()\n",
        "\n",
        "                # Create node attributes\n",
        "                attrs = {\n",
        "                    'timestamp': node_data['timestamp'],\n",
        "                    'embedding': embedding[0],  # First element (batch dimension)\n",
        "                    'hr': node_data['hr'],\n",
        "                    'spo2': node_data['spo2'],\n",
        "                    'activity_level': node_data['activity_level'],\n",
        "                    'sleep_stage': node_data['sleep_stage'],\n",
        "                    'stress': node_data['stress'],\n",
        "                    'hrv': node_data['hrv'],\n",
        "                }\n",
        "\n",
        "                # Add node\n",
        "                G_temp.add_node(i, **attrs)\n",
        "\n",
        "            # Add sequential edges\n",
        "            for i in range(len(embeddings) - 1):\n",
        "                G_temp.add_edge(\n",
        "                    i, i+1,\n",
        "                    edge_type='sequential',\n",
        "                    weight=1.0\n",
        "                )\n",
        "\n",
        "            # Step 3: Run GNN inference\n",
        "            afib_model = self.models['afib']\n",
        "            afib_model.eval()\n",
        "\n",
        "            # Prepare input for GNN\n",
        "            from torch_geometric.data import Data, Batch\n",
        "\n",
        "            risk_scores = []\n",
        "            timestamps = []\n",
        "\n",
        "            for node_idx in G_temp.nodes():\n",
        "                # Create a subgraph centered around this node\n",
        "                neighbors = list(G_temp.neighbors(node_idx))\n",
        "                subgraph_nodes = [node_idx] + neighbors\n",
        "\n",
        "                # Skip if not enough nodes\n",
        "                if len(subgraph_nodes) < 3:\n",
        "                    continue\n",
        "\n",
        "                subgraph = G_temp.subgraph(subgraph_nodes)\n",
        "\n",
        "                # Create PyG data object\n",
        "                node_mapping = {node: i for i, node in enumerate(subgraph_nodes)}\n",
        "\n",
        "                # Get node features\n",
        "                node_feats = []\n",
        "                for node in subgraph_nodes:\n",
        "                    embedding = G_temp.nodes[node]['embedding']\n",
        "                    other_feats = [\n",
        "                        G_temp.nodes[node]['hr'],\n",
        "                        G_temp.nodes[node]['spo2'],\n",
        "                        G_temp.nodes[node]['activity_level'],\n",
        "                        G_temp.nodes[node]['sleep_stage'],\n",
        "                        G_temp.nodes[node]['stress'],\n",
        "                        G_temp.nodes[node]['hrv']\n",
        "                    ]\n",
        "                    all_feats = np.concatenate([embedding, other_feats])\n",
        "                    node_feats.append(all_feats)\n",
        "\n",
        "                # Convert to tensor\n",
        "                x = torch.tensor(np.array(node_feats), dtype=torch.float)\n",
        "\n",
        "                # Get edges\n",
        "                edge_index = []\n",
        "                edge_attr = []\n",
        "\n",
        "                for u, v, data in subgraph.edges(data=True):\n",
        "                    edge_index.append([node_mapping[u], node_mapping[v]])\n",
        "\n",
        "                    edge_type = 1.0 if data.get('edge_type') == 'sequential' else 0.0\n",
        "                    is_virtual = 1.0 if data.get('is_virtual', False) else 0.0\n",
        "                    weight = data.get('weight', 1.0)\n",
        "\n",
        "                    edge_attr.append([edge_type, is_virtual, weight])\n",
        "\n",
        "                # Convert to tensors\n",
        "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "                # Create data object\n",
        "                data = Data(\n",
        "                    x=x,\n",
        "                    edge_index=edge_index,\n",
        "                    edge_attr=edge_attr,\n",
        "                    num_nodes=len(subgraph_nodes)\n",
        "                )\n",
        "\n",
        "                # Run inference\n",
        "                with torch.no_grad():\n",
        "                    batch = Batch.from_data_list([data])\n",
        "                    risk = afib_model(batch).item()\n",
        "\n",
        "                # Store results\n",
        "                risk_scores.append(risk)\n",
        "                timestamps.append(G_temp.nodes[node_idx]['timestamp'])\n",
        "\n",
        "            # Step 4: Apply Bayesian personalization\n",
        "            calibrated_scores = []\n",
        "            for score in risk_scores:\n",
        "                calibrated = self.components['bayes'].calibrate_risk(score)\n",
        "                calibrated_scores.append(calibrated)\n",
        "\n",
        "            # Step 5: Create results dataframe\n",
        "            results = pd.DataFrame({\n",
        "                'timestamp': timestamps,\n",
        "                'risk_score': risk_scores,\n",
        "                'calibrated_risk': calibrated_scores,\n",
        "                'alert': [score > 0.5 for score in calibrated_scores]\n",
        "            })\n",
        "\n",
        "            print(f\"Processed {len(results)} windows, detected {sum(results['alert'])} potential issues.\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing new data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def simulate_real_time_monitoring(self, duration_minutes=60, interval_seconds=5):\n",
        "        \"\"\"\n",
        "        Simulate real-time monitoring with the GET-Phen framework\n",
        "\n",
        "        Args:\n",
        "            duration_minutes: Duration of simulation in minutes\n",
        "            interval_seconds: Interval between updates in seconds\n",
        "        \"\"\"\n",
        "        print(f\"\\n=== Simulating Real-Time Monitoring for {duration_minutes} minutes ===\")\n",
        "\n",
        "        # Get a slice of data to use for simulation\n",
        "        data = self.data['smartwatch_data'].copy()\n",
        "\n",
        "        # Determine starting point - use a period with some risk\n",
        "        risk_scores = self.data['risk_scores']\n",
        "        interesting_period = risk_scores[risk_scores['risk_score'] > 0.3].iloc[0]\n",
        "        interesting_timestamp = pd.to_datetime(interesting_period['timestamp'])\n",
        "\n",
        "        # Find closest timestamp in data\n",
        "        start_idx = data['timestamp'].searchsorted(interesting_timestamp)\n",
        "        start_idx = max(0, start_idx - 30)  # Back up a bit\n",
        "\n",
        "        # Take a slice of data\n",
        "        data_slice = data.iloc[start_idx:start_idx + duration_minutes]\n",
        "\n",
        "        if len(data_slice) < duration_minutes:\n",
        "            print(f\"Warning: Not enough data for {duration_minutes} minutes. Using {len(data_slice)} points.\")\n",
        "\n",
        "        # Setup live plot\n",
        "        plt.ion()\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), gridspec_kw={'height_ratios': [2, 1]})\n",
        "\n",
        "        # Storage for real-time data\n",
        "        live_timestamps = []\n",
        "        live_hrs = []\n",
        "        live_spo2s = []\n",
        "        live_risk_scores = []\n",
        "\n",
        "        # Simulation loop\n",
        "        try:\n",
        "            window_size = 10\n",
        "            buffer = []\n",
        "            risk_buffer = []\n",
        "\n",
        "            for i, (_, row) in enumerate(data_slice.iterrows()):\n",
        "                # Add to buffer\n",
        "                buffer.append(row)\n",
        "\n",
        "                # Process in windows\n",
        "                if len(buffer) >= window_size:\n",
        "                    # Create mini-batch\n",
        "                    mini_batch = pd.DataFrame(buffer)\n",
        "\n",
        "                    # Process through framework (simplified for simulation)\n",
        "                    # In a real implementation, this would call process_new_data\n",
        "\n",
        "                    # Get corresponding risk score from pre-computed data\n",
        "                    mini_idx = start_idx + i - window_size // 2\n",
        "                    if 0 <= mini_idx < len(risk_scores):\n",
        "                        risk = risk_scores.iloc[mini_idx]['calibrated_risk']\n",
        "                    else:\n",
        "                        risk = 0.1 + 0.1 * np.random.random()\n",
        "\n",
        "                    risk_buffer.append(risk)\n",
        "\n",
        "                    # Remove oldest from buffer\n",
        "                    buffer = buffer[-window_size:]\n",
        "\n",
        "                # Update visualization every few points\n",
        "                if i % max(1, int(5 / interval_seconds)) == 0 or i == len(data_slice) - 1:\n",
        "                    # Add latest data\n",
        "                    live_timestamps.append(row['timestamp'])\n",
        "                    live_hrs.append(row['hr'])\n",
        "                    live_spo2s.append(row['spo2'])\n",
        "\n",
        "                    if len(risk_buffer) > 0:\n",
        "                        live_risk_scores.append(risk_buffer[-1])\n",
        "                    elif len(live_risk_scores) > 0:\n",
        "                        live_risk_scores.append(live_risk_scores[-1])\n",
        "                    else:\n",
        "                        live_risk_scores.append(0)\n",
        "\n",
        "                    # Keep data series to a reasonable length\n",
        "                    max_points = 100\n",
        "                    if len(live_timestamps) > max_points:\n",
        "                        live_timestamps = live_timestamps[-max_points:]\n",
        "                        live_hrs = live_hrs[-max_points:]\n",
        "                        live_spo2s = live_spo2s[-max_points:]\n",
        "                        live_risk_scores = live_risk_scores[-max_points:]\n",
        "\n",
        "                    # Update plot\n",
        "                    ax1.clear()\n",
        "                    ax2.clear()\n",
        "\n",
        "                    # Risk score plot\n",
        "                    ax1.plot(live_timestamps, live_risk_scores, 'g-')\n",
        "                    ax1.set_title('GET-Phen Real-Time Health Monitoring')\n",
        "                    ax1.set_ylabel('Disease Risk Score')\n",
        "                    ax1.set_ylim(0, 1)\n",
        "                    ax1.axhline(y=0.5, color='r', linestyle='--', label='Alert Threshold')\n",
        "\n",
        "                    # Add alert markers\n",
        "                    alerts = [(ts, score) for ts, score in zip(live_timestamps, live_risk_scores) if score > 0.5]\n",
        "                    if alerts:\n",
        "                        alert_times, alert_scores = zip(*alerts)\n",
        "                        ax1.scatter(alert_times, alert_scores, color='red', s=80, marker='o', label='Alert')\n",
        "\n",
        "                    ax1.legend()\n",
        "                    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "                    # Physio plot\n",
        "                    ax2.plot(live_timestamps, live_hrs, 'r-', label='Heart Rate')\n",
        "                    ax2.set_ylabel('Heart Rate (bpm)')\n",
        "                    ax2.set_xlabel('Time')\n",
        "                    ax2.legend(loc='upper left')\n",
        "\n",
        "                    ax2b = ax2.twinx()\n",
        "                    ax2b.plot(live_timestamps, live_spo2s, 'b-', alpha=0.7, label='SpO2')\n",
        "                    ax2b.set_ylabel('SpO2 (%)')\n",
        "                    ax2b.set_ylim(90, 100)\n",
        "                    ax2b.legend(loc='upper right')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.draw()\n",
        "                    plt.pause(0.001)\n",
        "\n",
        "                # Pause to simulate real-time\n",
        "                time.sleep(interval_seconds)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nSimulation stopped by user.\")\n",
        "\n",
        "        finally:\n",
        "            plt.ioff()\n",
        "            plt.show()\n",
        "\n",
        "            # Final summary\n",
        "            print(\"\\n=== Real-Time Simulation Summary ===\")\n",
        "            print(f\"Processed {len(data_slice)} data points\")\n",
        "            if len(live_risk_scores) > 0:\n",
        "                print(f\"Mean risk score: {np.mean(live_risk_scores):.4f}\")\n",
        "                print(f\"Max risk score: {np.max(live_risk_scores):.4f}\")\n",
        "                print(f\"Detected {sum(1 for score in live_risk_scores if score > 0.5)} potential issues\")\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        \"\"\"Generate a comprehensive summary report of all data and findings\"\"\"\n",
        "        print(\"\\n=== GET-Phen Summary Report ===\")\n",
        "\n",
        "        # User information\n",
        "        print(\"\\n== User Information ==\")\n",
        "        user = self.data['user_meta']\n",
        "        print(f\"Age: {user['age']}\")\n",
        "        print(f\"Gender: {user['gender']}\")\n",
        "        print(f\"BMI: {user['bmi']:.1f}\")\n",
        "        print(f\"Pre-existing conditions: {', '.join(user['conditions']) if user['conditions'] else 'None'}\")\n",
        "\n",
        "        # Data summary\n",
        "        print(\"\\n== Data Summary ==\")\n",
        "        data = self.data['smartwatch_data']\n",
        "        print(f\"Monitoring period: {data['timestamp'].min()} to {data['timestamp'].max()}\")\n",
        "        print(f\"Total duration: {(data['timestamp'].max() - data['timestamp'].min()).total_seconds() / 3600:.1f} hours\")\n",
        "        print(f\"Data points: {len(data)}\")\n",
        "\n",
        "        # Risk assessment summary\n",
        "        print(\"\\n== Risk Assessment ==\")\n",
        "        risk_scores = self.data['risk_scores']\n",
        "        print(f\"Mean risk score: {risk_scores['calibrated_risk'].mean():.4f}\")\n",
        "        print(f\"Potential disease events detected: {sum(risk_scores['alert'])}\")\n",
        "\n",
        "        # Health metrics summary\n",
        "        print(\"\\n== Health Metrics Summary ==\")\n",
        "        print(f\"Mean heart rate: {data['hr'].mean():.1f} bpm\")\n",
        "        print(f\"Mean SpO2: {data['spo2'].mean():.1f}%\")\n",
        "        print(f\"Mean HRV: {data['hrv'].mean():.1f} ms\")\n",
        "        print(f\"Mean stress level: {data['stress'].mean():.1f}\")\n",
        "\n",
        "        # Sleep summary\n",
        "        sleep_stats = data['sleep_stage'].value_counts()\n",
        "        total_sleep = sum(sleep_stats[i] for i in [1, 2, 3] if i in sleep_stats.index)\n",
        "        total_time = len(data)\n",
        "\n",
        "        print(\"\\n== Sleep Summary ==\")\n",
        "        print(f\"Total sleep time: {total_sleep} minutes ({total_sleep/60:.1f} hours)\")\n",
        "        print(f\"Percentage of time asleep: {100*total_sleep/total_time:.1f}%\")\n",
        "\n",
        "        for stage, label in zip([0, 1, 2, 3], ['Awake', 'Light sleep', 'Deep sleep', 'REM sleep']):\n",
        "            if stage in sleep_stats.index:\n",
        "                print(f\"{label}: {sleep_stats[stage]} minutes ({100*sleep_stats[stage]/total_time:.1f}%)\")\n",
        "\n",
        "        # High risk periods\n",
        "        print(\"\\n== High Risk Periods ==\")\n",
        "        high_risk = risk_scores[risk_scores['alert']].copy()\n",
        "\n",
        "        if len(high_risk) > 0:\n",
        "            # Group consecutive alerts\n",
        "            high_risk['alert_group'] = (high_risk['alert'] != high_risk['alert'].shift()).cumsum()\n",
        "            alert_groups = high_risk.groupby('alert_group')\n",
        "\n",
        "            for i, (_, group) in enumerate(alert_groups):\n",
        "                start_time = pd.to_datetime(group['timestamp'].min())\n",
        "                end_time = pd.to_datetime(group['timestamp'].max())\n",
        "                duration = (end_time - start_time).total_seconds() / 60\n",
        "                max_risk = group['calibrated_risk'].max()\n",
        "\n",
        "                print(f\"Alert {i+1}: {start_time.strftime('%Y-%m-%d %H:%M')} - {end_time.strftime('%H:%M')}\")\n",
        "                print(f\"  Duration: {duration:.1f} minutes\")\n",
        "                print(f\"  Maximum risk score: {max_risk:.4f}\")\n",
        "        else:\n",
        "            print(\"No high risk periods detected.\")\n",
        "\n",
        "        # Save report to file\n",
        "        with open('getphen_report.txt', 'w') as f:\n",
        "            f.write(\"GET-Phen Framework Summary Report\\n\")\n",
        "            f.write(\"================================\\n\\n\")\n",
        "\n",
        "            f.write(\"User Information\\n\")\n",
        "            f.write(\"---------------\\n\")\n",
        "            f.write(f\"Age: {user['age']}\\n\")\n",
        "            f.write(f\"Gender: {user['gender']}\\n\")\n",
        "            f.write(f\"BMI: {user['bmi']:.1f}\\n\")\n",
        "            f.write(f\"Pre-existing conditions: {', '.join(user['conditions']) if user['conditions'] else 'None'}\\n\\n\")\n",
        "\n",
        "            # Add more sections similar to the printed output\n",
        "\n",
        "        print(\"\\nFull report saved to 'getphen_report.txt'\")\n",
        "\n",
        "# Create GET-Phen framework\n",
        "getphen = GETPhen(load_pretrained=True)\n",
        "\n",
        "# Run demo\n",
        "demo_results = getphen.run_end_to_end_demo(demo_length_hours=24, real_time=False)\n",
        "\n",
        "# Generate summary report\n",
        "getphen.generate_summary_report()\n",
        "\n",
        "# For interactive demonstration, uncomment:\n",
        "# getphen.simulate_real_time_monitoring(duration_minutes=10, interval_seconds=0.5)\n",
        "\n",
        "print(\"\\n=== GET-Phen Framework Demo Completed ===\")\n",
        "print(\"All modules have been successfully integrated and demonstrated.\")"
      ],
      "metadata": {
        "id": "sZTJ8ELTKzIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output, HTML, display\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def print_header():\n",
        "    \"\"\"Print a fancy header for GET-Phen\"\"\"\n",
        "    header = \"\"\"\n",
        "    ██████╗ ███████╗████████╗   ██████╗ ██╗  ██╗███████╗███╗   ██╗\n",
        "    ██╔════╝ ██╔════╝╚══██╔══╝   ██╔══██╗██║  ██║██╔════╝████╗  ██║\n",
        "    ██║  ███╗█████╗     ██║█████╗██████╔╝███████║█████╗  ██╔██╗ ██║\n",
        "    ██║   ██║██╔══╝     ██║╚════╝██╔═══╝ ██╔══██║██╔══╝  ██║╚██╗██║\n",
        "    ╚██████╔╝███████╗   ██║      ██║     ██║  ██║███████╗██║ ╚████║\n",
        "    ╚═════╝ ╚══════╝   ╚═╝      ╚═╝     ╚═╝  ╚═╝╚══════╝╚═╝  ╚═══╝\n",
        "\n",
        "    Graph-Enabled Temporal Phenotyping Framework\n",
        "    ------------------------------------------\n",
        "    A framework for disease detection from smartwatch data\n",
        "    \"\"\"\n",
        "    print(header)\n",
        "\n",
        "def check_dependencies():\n",
        "    \"\"\"Check if all dependencies are installed\"\"\"\n",
        "    print(\"Checking dependencies...\")\n",
        "\n",
        "    required_packages = [\n",
        "        'torch', 'torch_geometric', 'transformers', 'networkx',\n",
        "        'numpy', 'pandas', 'scikit-learn', 'matplotlib', 'seaborn',\n",
        "        'plotly', 'tqdm'\n",
        "    ]\n",
        "\n",
        "    missing_packages = []\n",
        "\n",
        "    for package in required_packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"✓ {package}\")\n",
        "        except ImportError:\n",
        "            missing_packages.append(package)\n",
        "            print(f\"✗ {package}\")\n",
        "\n",
        "    if missing_packages:\n",
        "        print(\"\\nInstalling missing packages...\")\n",
        "        import subprocess\n",
        "\n",
        "        for package in missing_packages:\n",
        "            print(f\"Installing {package}...\")\n",
        "            if package == 'torch_geometric':\n",
        "                # Special installation for PyG\n",
        "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'torch-scatter', 'torch-sparse', 'torch-cluster', 'torch-geometric'])\n",
        "            else:\n",
        "                subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
        "\n",
        "        print(\"\\nAll dependencies installed!\")\n",
        "    else:\n",
        "        print(\"\\nAll dependencies are already installed!\")\n",
        "\n",
        "def run_module(module_number, module_name, file_path):\n",
        "    \"\"\"Run a specific GET-Phen module\"\"\"\n",
        "    print(f\"\\n=== Running Module {module_number}: {module_name} ===\")\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: File {file_path} not found!\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Run the module\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Execute module\n",
        "        exec(open(file_path).read())\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        print(f\"\\n✓ Module {module_number} completed successfully! ({duration:.2f} seconds)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error running module {module_number}: {e}\")\n",
        "        return False\n",
        "\n",
        "def run_pipeline(start_module=0, end_module=10):\n",
        "    \"\"\"Run the full GET-Phen pipeline\"\"\"\n",
        "    modules = [\n",
        "        (0, \"Environment Setup\", \"setup_environment.py\"),\n",
        "        (1, \"Data Ingestion\", \"data_ingestion.py\"),\n",
        "        (2, \"Self-Supervised Pretraining Model\", \"self_supervised_model.py\"),\n",
        "        (3, \"Generate Embeddings\", \"generate_embeddings.py\"),\n",
        "        (4, \"Graph Construction\", \"graph_construction.py\"),\n",
        "        (5, \"Digital Twin Augmentation\", \"digital_twin.py\"),\n",
        "        (6, \"Disease-Motif GNN\", \"disease_motif_gnn.py\"),\n",
        "        (7, \"Online Inference & Sliding Window\", \"online_inference.py\"),\n",
        "        (8, \"Bayesian Updating\", \"bayesian_updating.py\"),\n",
        "        (9, \"Visualization and Monitoring\", \"visualization_monitoring.py\"),\n",
        "        (10, \"Final Integration\", \"final_integration.py\")\n",
        "    ]\n",
        "\n",
        "    # Filter modules to run\n",
        "    modules_to_run = [m for m in modules if start_module <= m[0] <= end_module]\n",
        "\n",
        "    print(f\"Running {len(modules_to_run)} modules...\\n\")\n",
        "\n",
        "    # Run each module\n",
        "    results = []\n",
        "\n",
        "    for module_num, module_name, file_path in modules_to_run:\n",
        "        success = run_module(module_num, module_name, file_path)\n",
        "        results.append((module_num, module_name, success))\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n=== Pipeline Execution Summary ===\")\n",
        "\n",
        "    for module_num, module_name, success in results:\n",
        "        status = \"✓\" if success else \"✗\"\n",
        "        print(f\"{status} Module {module_num}: {module_name}\")\n",
        "\n",
        "    # Check if all modules succeeded\n",
        "    all_success = all(success for _, _, success in results)\n",
        "\n",
        "    if all_success:\n",
        "        print(\"\\n✓ All modules completed successfully!\")\n",
        "    else:\n",
        "        print(\"\\n✗ Some modules failed. Check the errors above.\")\n",
        "\n",
        "def display_menu():\n",
        "    \"\"\"Display the interactive menu\"\"\"\n",
        "    print_header()\n",
        "\n",
        "    print(\"\\nWelcome to the GET-Phen Framework!\")\n",
        "    print(\"This launcher helps you run the various components of the framework.\\n\")\n",
        "\n",
        "    menu = \"\"\"\n",
        "    === Main Menu ===\n",
        "    1. Run Full Pipeline\n",
        "    2. Run Specific Module\n",
        "    3. Run Demo (requires all modules to be run first)\n",
        "    4. Check Dependencies\n",
        "    5. Exit\n",
        "    \"\"\"\n",
        "\n",
        "    print(menu)\n",
        "\n",
        "    choice = input(\"Enter your choice (1-5): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        clear_output()\n",
        "        print_header()\n",
        "        print(\"\\n=== Running Full Pipeline ===\")\n",
        "        check_dependencies()\n",
        "        run_pipeline()\n",
        "    elif choice == '2':\n",
        "        clear_output()\n",
        "        print_header()\n",
        "\n",
        "        modules = [\n",
        "            (0, \"Environment Setup\"),\n",
        "            (1, \"Data Ingestion\"),\n",
        "            (2, \"Self-Supervised Pretraining Model\"),\n",
        "            (3, \"Generate Embeddings\"),\n",
        "            (4, \"Graph Construction\"),\n",
        "            (5, \"Digital Twin Augmentation\"),\n",
        "            (6, \"Disease-Motif GNN\"),\n",
        "            (7, \"Online Inference & Sliding Window\"),\n",
        "            (8, \"Bayesian Updating\"),\n",
        "            (9, \"Visualization and Monitoring\"),\n",
        "            (10, \"Final Integration\")\n",
        "        ]\n",
        "\n",
        "        print(\"\\n=== Available Modules ===\")\n",
        "        for num, name in modules:\n",
        "            print(f\"{num}. {name}\")\n",
        "\n",
        "        module = input(\"\\nEnter module number to run: \")\n",
        "        try:\n",
        "            module_num = int(module)\n",
        "            if 0 <= module_num <= 10:\n",
        "                module_path = f\"{modules[module_num][1].lower().replace(' ', '_').replace('-', '_')}.py\"\n",
        "                clear_output()\n",
        "                print_header()\n",
        "                check_dependencies()\n",
        "                run_module(module_num, modules[module_num][1], module_path)\n",
        "            else:\n",
        "                print(\"Invalid module number!\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number!\")\n",
        "    elif choice == '3':\n",
        "        clear_output()\n",
        "        print_header()\n",
        "        print(\"\\n=== Running GET-Phen Demo ===\")\n",
        "\n",
        "        # Check if final_integration.py exists\n",
        "        if not os.path.exists('final_integration.py'):\n",
        "            print(\"Error: final_integration.py not found! Please run the full pipeline first.\")\n",
        "        else:\n",
        "            try:\n",
        "                from final_integration import GETPhen\n",
        "\n",
        "                print(\"Creating GET-Phen framework...\")\n",
        "                getphen = GETPhen(load_pretrained=True)\n",
        "\n",
        "                print(\"\\nChoose demo type:\")\n",
        "                print(\"1. Standard Demo\")\n",
        "                print(\"2. Real-time Simulation\")\n",
        "\n",
        "                demo_choice = input(\"Enter choice (1-2): \")\n",
        "\n",
        "                if demo_choice == '1':\n",
        "                    # Run standard demo\n",
        "                    getphen.run_end_to_end_demo(demo_length_hours=24, real_time=False)\n",
        "                elif demo_choice == '2':\n",
        "                    # Run real-time simulation\n",
        "                    getphen.simulate_real_time_monitoring(duration_minutes=10, interval_seconds=0.5)\n",
        "                else:\n",
        "                    print(\"Invalid choice!\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error running demo: {e}\")\n",
        "                print(\"Please make sure all modules have been run successfully first.\")\n",
        "    elif choice == '4':\n",
        "        clear_output()\n",
        "        print_header()\n",
        "        check_dependencies()\n",
        "    elif choice == '5':\n",
        "        print(\"\\nExiting GET-Phen Framework. Goodbye!\")\n",
        "        return\n",
        "    else:\n",
        "        print(\"Invalid choice, please try again!\")\n",
        "\n",
        "    # Return to menu after action completes\n",
        "    input(\"\\nPress Enter to return to the main menu...\")\n",
        "    clear_output()\n",
        "    display_menu()\n",
        "\n",
        "# Run the menu\n",
        "if __name__ == \"__main__\":\n",
        "    display_menu()"
      ],
      "metadata": {
        "id": "0g2dvT8OK0YN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}